{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e1e44ef-f8f5-42dc-9cca-8973263f0e5f",
   "metadata": {},
   "source": [
    "# Here I run loop through all DMAs to find bundles for spatial discontinuity design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "d505ff49-c973-4358-9b37-7b2decff908e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed DMA: New York, NY\n",
      "Processed DMA: Los Angeles, CA\n",
      "Processed DMA: Chicago, IL\n",
      "Processed DMA: Dallas-Ft. Worth, TX\n",
      "Processed DMA: Philadelphia, PA\n",
      "Processed DMA: Houston, TX\n",
      "Processed DMA: Atlanta, GA\n",
      "Processed DMA: Washington, DC\n",
      "Processed DMA: Boston, MA\n",
      "Processed DMA: San Francisco-Oakland-San Jose, CA\n",
      "Processed DMA: Tampa-St. Petersburg\n",
      "Processed DMA: Phoenix, AZ\n",
      "Processed DMA: Seattle-Tacoma, WA\n",
      "Processed DMA: Detroit, MI\n",
      "Processed DMA: Orlando-Daytona Beach-Melbourne, FL\n",
      "Processed DMA: Minneapolis-St. Paul, MN\n",
      "Processed DMA: Denver, CO\n",
      "Processed DMA: Miami-Fort Lauderdale, FL\n",
      "Processed DMA: Cleveland-Akron\n",
      "Processed DMA: Sacramento-Stockton-Modesto, CA\n",
      "Processed DMA: Charlotte, NC\n",
      "Processed DMA: Raleigh-Durham\n",
      "Processed DMA: Portland, OR\n",
      "Processed DMA: St. Louis, MO\n",
      "Processed DMA: Indianapolis, IN\n",
      "Processed DMA: Nashville, TN\n",
      "Processed DMA: Pittsburgh, PA\n",
      "Processed DMA: Salt Lake City, UT\n",
      "Processed DMA: Baltimore, MD\n",
      "Processed DMA: San Diego, CA\n",
      "Processed DMA: San Antonio, TX\n",
      "Processed DMA: Hartford & New Haven, CT\n",
      "Processed DMA: Kansas City, MO\n",
      "Processed DMA: Austin, TX\n",
      "Processed DMA: Columbus, OH\n",
      "Processed DMA: Greenville-Spartanburg, SC-Asheville, NC-Anderson,SC\n",
      "Processed DMA: Cincinnati, OH\n",
      "Processed DMA: Milwaukee, WI\n",
      "Processed DMA: West Palm Beach-Ft. Pierce, FL\n",
      "Processed DMA: Las Vegas, NV\n",
      "Processed DMA: Jacksonville, FL\n",
      "Processed DMA: Harrisburg-Lancaster-Lebanon-York, PA\n",
      "Processed DMA: Grand Rapids-Kalamazoo-Battle Creek, MI\n",
      "Processed DMA: Norfolk-Portsmouth-Newport News, VA\n",
      "Processed DMA: Birmingham\n",
      "Processed DMA: Greensboro-High Point-Winston Salem, NC\n",
      "Processed DMA: Oklahoma City, OK\n",
      "Processed DMA: Albuquerque-Santa Fe, NM\n",
      "Processed DMA: Louisville, KY\n",
      "Processed DMA: New Orleans, LA\n",
      "Processed DMA: Memphis, TN\n",
      "Processed DMA: Providence, RI-New Bedford, MA\n",
      "Processed DMA: Ft. Myers-Naples, FL\n",
      "Processed DMA: Buffalo, NY\n",
      "Processed DMA: Fresno-Visalia, CA\n",
      "Processed DMA: Richmond-Petersburg, VA\n",
      "Processed DMA: Mobile, AL-Pensacola\n",
      "Processed DMA: Little Rock-Pine Bluff, AR\n",
      "Processed DMA: Wilkes Barre-Scranton, PA\n",
      "Processed DMA: Knoxville, TN\n",
      "Processed DMA: Tulsa, OK\n",
      "Processed DMA: Albany-Schenectady-Troy, NY\n",
      "Processed DMA: Lexington, KY\n",
      "Processed DMA: Dayton, OH\n",
      "Processed DMA: Tucson\n",
      "Processed DMA: Spokane, WA\n",
      "Processed DMA: Des Moines-Ames, IA\n",
      "Processed DMA: Green Bay-Appleton, WI\n",
      "Processed DMA: Roanoke-Lynchburg, VA\n",
      "Processed DMA: Wichita-Hutchinson, KS Plus\n",
      "Processed DMA: Flint-Saginaw-Bay City, MI\n",
      "Processed DMA: Omaha, NE\n",
      "Processed DMA: Springfield, MO\n",
      "Processed DMA: Huntsville-Decatur\n",
      "Processed DMA: Columbia, SC\n",
      "Processed DMA: Madison, WI\n",
      "Processed DMA: Portland-Auburn, ME\n",
      "Processed DMA: Rochester, NY\n",
      "Processed DMA: Harlingen-Weslaco-Brownsville-McAllen, TX\n",
      "Processed DMA: Toledo, OH\n",
      "Processed DMA: Charleston-Huntington, WV\n",
      "Processed DMA: Waco-Temple-Bryan, TX\n",
      "Processed DMA: Savannah, GA\n",
      "Processed DMA: Charleston, SC\n",
      "Processed DMA: Chattanooga, TN\n",
      "Processed DMA: Colorado Springs-Pueblo, CO\n",
      "Processed DMA: Syracuse, NY\n",
      "Processed DMA: El Paso, TX\n",
      "Processed DMA: Paducah, KY-Cape Girardeau, MO-Harrisburg, IL\n",
      "Processed DMA: Shreveport, LA\n",
      "Processed DMA: Champaign & Springfield-Decatur, IL\n",
      "Processed DMA: Burlington, VT-Plattsburgh, NY\n",
      "Processed DMA: Cedar Rapids-Waterloo-Iowa City & Dubuque, IA\n",
      "Processed DMA: Baton Rouge, LA\n",
      "Processed DMA: Ft. Smith-Fayetteville-Springdale-Rogers, AR\n",
      "Processed DMA: Myrtle Beach-Florence, SC\n",
      "Processed DMA: Boise, ID\n",
      "Processed DMA: Jackson, MS\n",
      "Processed DMA: South Bend-Elkhart, IN\n",
      "Processed DMA: Tri-Cities, TN-VA\n",
      "Processed DMA: Greenville-New Bern-Washington, NC\n",
      "Processed DMA: Reno, NV\n",
      "Processed DMA: Davenport, IA-Rock Island-Moline, IL\n",
      "Processed DMA: Tallahassee, FL-Thomasville, GA\n",
      "Processed DMA: Tyler-Longview\n",
      "Processed DMA: Lincoln & Hastings-Kearney, NE\n",
      "Processed DMA: Augusta, GA\n",
      "Processed DMA: Evansville, IN\n",
      "Processed DMA: Ft. Wayne, IN\n",
      "Processed DMA: Sioux Falls\n",
      "Processed DMA: Johnstown-Altoona, PA\n",
      "Processed DMA: Fargo-Valley City, ND\n",
      "Processed DMA: Yakima-Pasco-Richland-Kennewick, WA\n",
      "Processed DMA: Springfield-Holyoke, MA\n",
      "Processed DMA: Traverse City-Cadillac, MI\n",
      "Processed DMA: Lansing, MI\n",
      "Processed DMA: Youngstown, OH\n",
      "Processed DMA: Macon, GA\n",
      "Processed DMA: Eugene, OR\n",
      "Processed DMA: Montgomery-Selma, AL\n",
      "Processed DMA: Peoria-Bloomington, IL\n",
      "Processed DMA: Santa Barbara-Santa Maria-San Luis Obispo, CA\n",
      "Processed DMA: Lafayette, LA\n",
      "Skipping DMA Bakersfield, CA due to empty final counties.\n",
      "Processed DMA: Wilmington, NC\n",
      "Processed DMA: Columbus, GA\n",
      "Processed DMA: Monterey-Salinas, CA\n",
      "Processed DMA: La Crosse-Eau Claire, WI\n",
      "Processed DMA: Corpus Christi, TX\n",
      "Processed DMA: Salisbury, MD\n",
      "Processed DMA: Amarillo, TX\n",
      "Processed DMA: Wausau-Rhinelander, WI\n",
      "Processed DMA: Columbus-Tupelo-West Point, MS\n",
      "Processed DMA: Columbia-Jefferson City, MO\n",
      "Processed DMA: Chico-Redding, CA\n",
      "Processed DMA: Rockford, IL\n",
      "Processed DMA: Duluth, MN-Superior, WI\n",
      "Processed DMA: Medford-Klamath Falls, OR\n",
      "Processed DMA: Lubbock, TX\n",
      "Processed DMA: Topeka, KS\n",
      "Processed DMA: Monroe, LA-El Dorado, AR\n",
      "Processed DMA: Beaumont-Port Arthur, TX\n",
      "Processed DMA: Odessa-Midland, TX\n",
      "Skipping DMA Palm Springs, CA due to empty final counties.\n",
      "Processed DMA: Minot-Bismarck-Dickinson\n",
      "Processed DMA: Panama City, FL\n",
      "Processed DMA: Sioux City, IA\n",
      "Processed DMA: Wichita Falls, TX-Lawton, OK\n",
      "Processed DMA: Joplin, MO-Pittsburg, KS\n",
      "Processed DMA: Albany, GA\n",
      "Processed DMA: Rochester, MN-Mason City, IA-Austin, MN\n",
      "Processed DMA: Erie, PA\n",
      "Processed DMA: Idaho Falls-Pocatello, ID\n",
      "Processed DMA: Bangor, ME\n",
      "Processed DMA: Gainesville, FL\n",
      "Processed DMA: Biloxi-Gulfport, MS\n",
      "Processed DMA: Terre Haute, IN\n",
      "Processed DMA: Sherman, TX-Ada, OK\n",
      "Processed DMA: Missoula, MT\n",
      "Processed DMA: Binghamton, NY\n",
      "Processed DMA: Wheeling, WV-Steubenville, OH\n",
      "Processed DMA: Yuma, AZ-El Centro, CA\n",
      "Processed DMA: Billings, MT\n",
      "Processed DMA: Abilene-Sweetwater, TX\n",
      "Processed DMA: Bluefield-Beckley-Oak Hill, WV\n",
      "Processed DMA: Hattiesburg-Laurel, MS\n",
      "Processed DMA: Rapid City, SD\n",
      "Processed DMA: Dothan, AL\n",
      "Processed DMA: Utica, NY\n",
      "Processed DMA: Clarksburg-Weston, WV\n",
      "Processed DMA: Harrisonburg, VA\n",
      "Processed DMA: Jackson, TN\n",
      "Processed DMA: Quincy, IL-Hannibal, MO-Keokuk, IA\n",
      "Processed DMA: Charlottesville, VA\n",
      "Processed DMA: Lake Charles, LA\n",
      "Processed DMA: Elmira, NY\n",
      "Processed DMA: Watertown, NY\n",
      "Processed DMA: Bowling Green, KY\n",
      "Processed DMA: Marquette, MI\n",
      "Processed DMA: Jonesboro, AR\n",
      "Processed DMA: Alexandria, LA\n",
      "Processed DMA: Laredo, TX\n",
      "Processed DMA: Butte-Bozeman, MT\n",
      "Processed DMA: Bend, OR\n",
      "Processed DMA: Grand Junction-Montrose, CO\n",
      "Processed DMA: Twin Falls, ID\n",
      "Processed DMA: Lafayette, IN\n",
      "Processed DMA: Lima, OH\n",
      "Processed DMA: Great Falls, MT\n",
      "Processed DMA: Meridian, MS\n",
      "Processed DMA: Cheyenne, WY-Scottsbluff, NE\n",
      "Processed DMA: Parkersburg, WV\n",
      "Processed DMA: Greenwood-Greenville, MS\n",
      "Processed DMA: Eureka, CA\n",
      "Processed DMA: San Angelo, TX\n",
      "Processed DMA: Casper-Riverton, WY\n",
      "Processed DMA: Mankato, MN\n",
      "Processed DMA: Ottumwa, IA-Kirksville, MO\n",
      "Processed DMA: St. Joseph, MO\n",
      "Processed DMA: Zanesville, OH\n",
      "Processed DMA: Victoria, TX\n",
      "Processed DMA: Helena, MT\n",
      "Processed DMA: Presque Isle, ME\n",
      "Processed DMA: Alpena, MI\n",
      "Processed DMA: North Platte, NE\n",
      "Processed DMA: Glendive, MT\n"
     ]
    }
   ],
   "source": [
    "custom_dma_order = [\n",
    "    \"New York, NY\",\n",
    "    \"Los Angeles, CA\",\n",
    "    \"Chicago, IL\",\n",
    "    \"Dallas-Ft. Worth, TX\",\n",
    "    \"Philadelphia, PA\",\n",
    "    \"Houston, TX\",\n",
    "    \"Atlanta, GA\",\n",
    "    \"Washington, DC (Hagerstown, MD)\",\n",
    "    \"Boston, MA (Manchester, NH)\",\n",
    "    \"San Francisco-Oakland-San Jose, CA\",\n",
    "    \"Tampa-St. Petersburg (Sarasota), FL\",\n",
    "    \"Phoenix, AZ\",\n",
    "    \"Seattle-Tacoma, WA\",\n",
    "    \"Detroit, MI\",\n",
    "    \"Orlando-Daytona Beach-Melbourne, FL\",\n",
    "    \"Minneapolis-St. Paul, MN\",\n",
    "    \"Denver, CO\",\n",
    "    \"Miami-Fort Lauderdale, FL\",\n",
    "    \"Cleveland-Akron (Canton), OH\",\n",
    "    \"Sacramento-Stockton-Modesto, CA\",\n",
    "    \"Charlotte, NC\",\n",
    "    \"Raleigh-Durham (Fayetteville), NC\",\n",
    "    \"Portland, OR\",\n",
    "    \"St. Louis, MO\",\n",
    "    \"Indianapolis, IN\",\n",
    "    \"Nashville, TN\",\n",
    "    \"Pittsburgh, PA\",\n",
    "    \"Salt Lake City, UT\",\n",
    "    \"Baltimore, MD\",\n",
    "    \"San Diego, CA\",\n",
    "    \"San Antonio, TX\",\n",
    "    \"Hartford & New Haven, CT\",\n",
    "    \"Kansas City, MO\",\n",
    "    \"Austin, TX\",\n",
    "    \"Columbus, OH\",\n",
    "    \"Greenville-Spartanburg, SC-Asheville, NC-Anderson,SC\",\n",
    "    \"Cincinnati, OH\",\n",
    "    \"Milwaukee, WI\",\n",
    "    \"West Palm Beach-Ft. Pierce, FL\",\n",
    "    \"Las Vegas, NV\",\n",
    "    \"Jacksonville, FL\",\n",
    "    \"Harrisburg-Lancaster-Lebanon-York, PA\",\n",
    "    \"Grand Rapids-Kalamazoo-Battle Creek, MI\",\n",
    "    \"Norfolk-Portsmouth-Newport News, VA\",\n",
    "    \"Birmingham (Anniston and Tuscaloosa), AL\",\n",
    "    \"Greensboro-High Point-Winston Salem, NC\",\n",
    "    \"Oklahoma City, OK\",\n",
    "    \"Albuquerque-Santa Fe, NM\",\n",
    "    \"Louisville, KY\",\n",
    "    \"New Orleans, LA\",\n",
    "    \"Memphis, TN\",\n",
    "    \"Providence, RI-New Bedford, MA\",\n",
    "    \"Ft. Myers-Naples, FL\",\n",
    "    \"Buffalo, NY\",\n",
    "    \"Fresno-Visalia, CA\",\n",
    "    \"Richmond-Petersburg, VA\",\n",
    "    \"Mobile, AL-Pensacola (Ft. Walton Beach), FL\",\n",
    "    \"Little Rock-Pine Bluff, AR\",\n",
    "    \"Wilkes Barre-Scranton, PA\",\n",
    "    \"Knoxville, TN\",\n",
    "    \"Tulsa, OK\",\n",
    "    \"Albany-Schenectady-Troy, NY\",\n",
    "    \"Lexington, KY\",\n",
    "    \"Dayton, OH\",\n",
    "    \"Tucson (Sierra Vista), AZ\",\n",
    "    \"Spokane, WA\",\n",
    "    \"Des Moines-Ames, IA\",\n",
    "    \"Green Bay-Appleton, WI\",\n",
    "    \"Honolulu, HI\",\n",
    "    \"Roanoke-Lynchburg, VA\",\n",
    "    \"Wichita-Hutchinson, KS Plus\",\n",
    "    \"Flint-Saginaw-Bay City, MI\",\n",
    "    \"Omaha, NE\",\n",
    "    \"Springfield, MO\",\n",
    "    \"Huntsville-Decatur (Florence), AL\",\n",
    "    \"Columbia, SC\",\n",
    "    \"Madison, WI\",\n",
    "    \"Portland-Auburn, ME\",\n",
    "    \"Rochester, NY\",\n",
    "    \"Harlingen-Weslaco-Brownsville-McAllen, TX\",\n",
    "    \"Toledo, OH\",\n",
    "    \"Charleston-Huntington, WV\",\n",
    "    \"Waco-Temple-Bryan, TX\",\n",
    "    \"Savannah, GA\",\n",
    "    \"Charleston, SC\",\n",
    "    \"Chattanooga, TN\",\n",
    "    \"Colorado Springs-Pueblo, CO\",\n",
    "    \"Syracuse, NY\",\n",
    "    \"El Paso, TX\",\n",
    "    \"Paducah, KY-Cape Girardeau, MO-Harrisburg, IL\",\n",
    "    \"Shreveport, LA\",\n",
    "    \"Champaign & Springfield-Decatur, IL\",\n",
    "    \"Burlington, VT-Plattsburgh, NY\",\n",
    "    \"Cedar Rapids-Waterloo-Iowa City & Dubuque, IA\",\n",
    "    \"Baton Rouge, LA\",\n",
    "    \"Ft. Smith-Fayetteville-Springdale-Rogers, AR\",\n",
    "    \"Myrtle Beach-Florence, SC\",\n",
    "    \"Boise, ID\",\n",
    "    \"Jackson, MS\",\n",
    "    \"South Bend-Elkhart, IN\",\n",
    "    \"Tri-Cities, TN-VA\",\n",
    "    \"Greenville-New Bern-Washington, NC\",\n",
    "    \"Reno, NV\",\n",
    "    \"Davenport, IA-Rock Island-Moline, IL\",\n",
    "    \"Tallahassee, FL-Thomasville, GA\",\n",
    "    \"Tyler-Longview(Lufkin & Nacogdoches), TX\",\n",
    "    \"Lincoln & Hastings-Kearney, NE\",\n",
    "    \"Augusta, GA\",\n",
    "    \"Evansville, IN\",\n",
    "    \"Ft. Wayne, IN\",\n",
    "    \"Sioux Falls (Mitchell), SD\",\n",
    "    \"Johnstown-Altoona, PA\",\n",
    "    \"Fargo-Valley City, ND\",\n",
    "    \"Yakima-Pasco-Richland-Kennewick, WA\",\n",
    "    \"Springfield-Holyoke, MA\",\n",
    "    \"Traverse City-Cadillac, MI\",\n",
    "    \"Lansing, MI\",\n",
    "    \"Youngstown, OH\",\n",
    "    \"Macon, GA\",\n",
    "    \"Eugene, OR\",\n",
    "    \"Montgomery-Selma, AL\",\n",
    "    \"Peoria-Bloomington, IL\",\n",
    "    \"Santa Barbara-Santa Maria-San Luis Obispo, CA\",\n",
    "    \"Lafayette, LA\",\n",
    "    \"Bakersfield, CA\",\n",
    "    \"Wilmington, NC\",\n",
    "    \"Columbus, GA\",\n",
    "    \"Monterey-Salinas, CA\",\n",
    "    \"La Crosse-Eau Claire, WI\",\n",
    "    \"Corpus Christi, TX\",\n",
    "    \"Salisbury, MD\",\n",
    "    \"Amarillo, TX\",\n",
    "    \"Wausau-Rhinelander, WI\",\n",
    "    \"Columbus-Tupelo-West Point, MS\",\n",
    "    \"Columbia-Jefferson City, MO\",\n",
    "    \"Chico-Redding, CA\",\n",
    "    \"Rockford, IL\",\n",
    "    \"Duluth, MN-Superior, WI\",\n",
    "    \"Medford-Klamath Falls, OR\",\n",
    "    \"Lubbock, TX\",\n",
    "    \"Topeka, KS\",\n",
    "    \"Monroe, LA-El Dorado, AR\",\n",
    "    \"Beaumont-Port Arthur, TX\",\n",
    "    \"Odessa-Midland, TX\",\n",
    "    \"Palm Springs, CA\",\n",
    "    \"Anchorage, AK\",\n",
    "    \"Minot-Bismarck-Dickinson(Williston), ND\",\n",
    "    \"Panama City, FL\",\n",
    "    \"Sioux City, IA\",\n",
    "    \"Wichita Falls, TX-Lawton, OK\",\n",
    "    \"Joplin, MO-Pittsburg, KS\",\n",
    "    \"Albany, GA\",\n",
    "    \"Rochester, MN-Mason City, IA-Austin, MN\",\n",
    "    \"Erie, PA\",\n",
    "    \"Idaho Falls-Pocatello, ID\",\n",
    "    \"Bangor, ME\",\n",
    "    \"Gainesville, FL\",\n",
    "    \"Biloxi-Gulfport, MS\",\n",
    "    \"Terre Haute, IN\",\n",
    "    \"Sherman, TX-Ada, OK\",\n",
    "    \"Missoula, MT\",\n",
    "    \"Binghamton, NY\",\n",
    "    \"Wheeling, WV-Steubenville, OH\",\n",
    "    \"Yuma, AZ-El Centro, CA\",\n",
    "    \"Billings, MT\",\n",
    "    \"Abilene-Sweetwater, TX\",\n",
    "    \"Bluefield-Beckley-Oak Hill, WV\",\n",
    "    \"Hattiesburg-Laurel, MS\",\n",
    "    \"Rapid City, SD\",\n",
    "    \"Dothan, AL\",\n",
    "    \"Utica, NY\",\n",
    "    \"Clarksburg-Weston, WV\",\n",
    "    \"Harrisonburg, VA\",\n",
    "    \"Jackson, TN\",\n",
    "    \"Quincy, IL-Hannibal, MO-Keokuk, IA\",\n",
    "    \"Charlottesville, VA\",\n",
    "    \"Lake Charles, LA\",\n",
    "    \"Elmira, NY\",\n",
    "    \"Watertown, NY\",\n",
    "    \"Bowling Green, KY\",\n",
    "    \"Marquette, MI\",\n",
    "    \"Jonesboro, AR\",\n",
    "    \"Alexandria, LA\",\n",
    "    \"Laredo, TX\",\n",
    "    \"Butte-Bozeman, MT\",\n",
    "    \"Bend, OR\",\n",
    "    \"Grand Junction-Montrose, CO\",\n",
    "    \"Twin Falls, ID\",\n",
    "    \"Lafayette, IN\",\n",
    "    \"Lima, OH\",\n",
    "    \"Great Falls, MT\",\n",
    "    \"Meridian, MS\",\n",
    "    \"Cheyenne, WY-Scottsbluff, NE\",\n",
    "    \"Parkersburg, WV\",\n",
    "    \"Greenwood-Greenville, MS\",\n",
    "    \"Eureka, CA\",\n",
    "    \"San Angelo, TX\",\n",
    "    \"Casper-Riverton, WY\",\n",
    "    \"Mankato, MN\",\n",
    "    \"Ottumwa, IA-Kirksville, MO\",\n",
    "    \"St. Joseph, MO\",\n",
    "    \"Fairbanks, AK\",\n",
    "    \"Zanesville, OH\",\n",
    "    \"Victoria, TX\",\n",
    "    \"Helena, MT\",\n",
    "    \"Presque Isle, ME\",\n",
    "    \"Juneau, AK\",\n",
    "    \"Alpena, MI\",\n",
    "    \"North Platte, NE\",\n",
    "    \"Glendive, MT\"\n",
    "]\n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from shapely.ops import unary_union\n",
    "from shapely.geometry import MultiPolygon\n",
    "from shapely.geometry import Point\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# File paths\n",
    "dma_fp = \"/Users/oliverraab/Desktop/CUNY/5_semester/Short_Project3/Data/DMA_data/nielsen-dma-master/nielsen_dma.json\"\n",
    "zip_shapefile_fp = \"/Users/oliverraab/Desktop/CUNY/5_semester/Short_Project3/Data/ZIP_code_coordinates_new/tl_2020_us_zcta520/tl_2020_us_zcta520.shp\"\n",
    "county_shapefile_fp = \"/Users/oliverraab/Desktop/CUNY/5_semester/Short_Project3/Data/County_coordinates_new/2018/cb_2018_us_county_500k/cb_2018_us_county_500k.shp\"\n",
    "zip_flags_fp = \"/Users/oliverraab/Desktop/CUNY/5_semester/Short_Project3/Analysis/ZIP_code_alternative_solution(contain_pyhon_file_with_DMA_calculation)/zip_duplicates3.dta\"\n",
    "\n",
    "# Load data and sort them from the largest to smallest DMA\n",
    "dmas = gpd.read_file(dma_fp).to_crs(\"EPSG:4269\")\n",
    "\n",
    "\n",
    "# Load data and sort them from the largest to smallest DMA\n",
    "# Convert to a Series with order indices\n",
    "dma_rank_series = pd.Series(range(len(custom_dma_order)), index=custom_dma_order)\n",
    "# Add a sort index column to `dmas`\n",
    "dmas[\"dma_sort_order\"] = dmas[\"dma1\"].map(dma_rank_series)\n",
    "# Drop DMAs not in your list (optional)\n",
    "dmas = dmas[dmas[\"dma_sort_order\"].notna()]\n",
    "\n",
    "# Sort the GeoDataFrame\n",
    "dmas = dmas.sort_values(\"dma_sort_order\").reset_index(drop=True)\n",
    "\n",
    "dmas[\"dma1\"] = dmas[\"dma1\"].str.replace(r\" \\(.*\", \"\", regex=True)\n",
    "dmas[\"dma1\"] = dmas[\"dma1\"].str.replace(r\"\\(.*\", \"\", regex=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#dmas2 = gpd.read_file(dma_fp).to_crs(\"EPSG:4269\")\n",
    "\n",
    "\n",
    "\n",
    "zip_gdf_master = gpd.read_file(zip_shapefile_fp).set_crs(\"EPSG:4269\")\n",
    "county_gdf = gpd.read_file(county_shapefile_fp).to_crs(\"EPSG:4269\")\n",
    "\n",
    "# Fix geometries\n",
    "dmas[\"geometry\"] = dmas[\"geometry\"].buffer(0)\n",
    "zip_gdf_master[\"geometry\"] = zip_gdf_master[\"geometry\"].buffer(0)\n",
    "county_gdf[\"geometry\"] = county_gdf[\"geometry\"].buffer(0)\n",
    "\n",
    "# Load ZIP flags\n",
    "original_zip_flags_df = pd.read_stata(zip_flags_fp)\n",
    "\n",
    "# Rename original columns\n",
    "original_zip_flags_df = original_zip_flags_df.rename(columns={\n",
    "    \"assigned_bundle\": \"assigned_bundle_1\",\n",
    "    \"overlap_class\": \"overlap_class_1\"\n",
    "})\n",
    "\n",
    "# Duplicate the renamed columns\n",
    "original_zip_flags_df[\"assigned_bundle_2\"] = original_zip_flags_df[\"assigned_bundle_1\"]\n",
    "original_zip_flags_df[\"overlap_class_2\"] = original_zip_flags_df[\"overlap_class_1\"]\n",
    "\n",
    "original_zip_flags_df[\"zipcode\"] = original_zip_flags_df[\"zipcode\"].astype(str).str.zfill(5)\n",
    "\n",
    "# To collect assignment results\n",
    "all_assignments = []\n",
    "\n",
    "final_matched = pd.DataFrame()\n",
    "final_all_counties = pd.DataFrame()\n",
    "final_all_counties2 = pd.DataFrame()\n",
    "premerge_matched_counties_F = pd.DataFrame()\n",
    "final_matched_counties_F = pd.DataFrame()\n",
    "\n",
    "\n",
    "\n",
    "# Loop over DMA names\n",
    "for dma_name in dmas[\"dma1\"].dropna().unique():\n",
    "    try:\n",
    "        ny_dma = dmas[dmas['dma1'].str.contains(re.escape(dma_name), case=False, na=False, regex=True)]\n",
    "        if ny_dma.empty:\n",
    "            continue\n",
    "    \n",
    "        # --- Identify adjacent DMAs and store for plotting only ---\n",
    "        projected_crs = \"EPSG:5070\"\n",
    "        dmas_proj = dmas.to_crs(projected_crs)\n",
    "        dmas_proj[\"geometry\"] = dmas_proj[\"geometry\"].buffer(0)\n",
    "        dmas_proj = dmas_proj[dmas_proj.is_valid]\n",
    "\n",
    "        ny_dma_proj = dmas_proj[dmas['dma1'].str.contains(dma_name, case=False, na=False)].copy()\n",
    "        ny_union = ny_dma_proj.unary_union\n",
    "\n",
    "        sindex = dmas_proj.sindex\n",
    "        adjacent_candidates = list(sindex.intersection(ny_union.buffer(0.1).bounds))\n",
    "        adjacent_dmas = dmas_proj.iloc[adjacent_candidates]\n",
    "        adjacent_dmas = adjacent_dmas[\n",
    "            (adjacent_dmas.geometry.touches(ny_union) | adjacent_dmas.geometry.intersects(ny_union)) &\n",
    "            (~adjacent_dmas[\"dma1\"].str.contains(dma_name, case=False, na=False))\n",
    "        ]\n",
    "        adjacent_dmas = adjacent_dmas.to_crs(\"EPSG:4269\")  # for plotting only\n",
    "\n",
    "        zip_flags_df = original_zip_flags_df.copy()\n",
    "        zip_gdf = zip_gdf_master.copy()\n",
    "        zip_gdf = zip_gdf[zip_gdf[\"ZCTA5CE20\"].isin(zip_flags_df[\"zipcode\"])]\n",
    "        zip_gdf = zip_gdf.to_crs(\"EPSG:4269\")\n",
    "\n",
    "        projected_crs = \"EPSG:5070\"\n",
    "        ny_dma_proj = ny_dma.to_crs(projected_crs)\n",
    "        county_proj = county_gdf.to_crs(projected_crs)\n",
    "        ny_dma_proj[\"geometry\"] = ny_dma_proj[\"geometry\"].buffer(0)\n",
    "        ny_dma_union = ny_dma_proj.unary_union\n",
    "        #if ny_dma_union is None or ny_dma_union.is_empty:\n",
    "        #    print(f\"Skipping DMA {dma_name} due to empty geometry union.\")\n",
    "        #    continue\n",
    "        ny_dma_union = ny_dma_union.buffer(0)\n",
    "\n",
    "        buffer_distance = 4828.03\n",
    "        outer = ny_dma_union.buffer(buffer_distance)\n",
    "        inner = ny_dma_union.buffer(-buffer_distance)\n",
    "        dma_band = outer.difference(inner)\n",
    "\n",
    "        band_counties = county_proj[county_proj.geometry.intersects(dma_band)].copy()\n",
    "        band_counties[\"area_total\"] = band_counties.geometry.area\n",
    "        band_counties[\"area_overlap\"] = band_counties.geometry.intersection(ny_dma_union).area\n",
    "        band_counties[\"pct_overlap\"] = band_counties[\"area_overlap\"] / band_counties[\"area_total\"]\n",
    "        \n",
    "        # Ensure DMAs are in same projection\n",
    "        dma_polygons = dmas_proj.copy()\n",
    "        \n",
    "        # Initialize column\n",
    "        band_counties[\"dma_name\"] = None\n",
    "\n",
    "        # Iterate over counties to assign DMA based on ≥60% area overlap\n",
    "        for idx, county_row in band_counties.iterrows():\n",
    "            county_geom = county_row.geometry\n",
    "            county_area = county_geom.area\n",
    "            for _, dma_row in dma_polygons.iterrows():\n",
    "                dma_geom = dma_row.geometry\n",
    "                intersection_area = county_geom.intersection(dma_geom).area\n",
    "                if (intersection_area / county_area) >= 0.60:\n",
    "                    band_counties.at[idx, \"dma_name\"] = dma_row[\"dma1\"]\n",
    "                    break  # stop once first DMA with ≥60% match is found                        \n",
    "\n",
    "        low = band_counties[band_counties[\"pct_overlap\"] < 0.15].copy()\n",
    "        high = band_counties[band_counties[\"pct_overlap\"] > 0.80].copy()\n",
    "        high_sindex = high.sindex\n",
    "        proximity_threshold = 1609.34\n",
    "        valid_low, valid_high = set(), set()\n",
    "\n",
    "        for i, low_row in low.iterrows():\n",
    "            candidates = list(high_sindex.intersection(low_row.geometry.buffer(proximity_threshold).bounds))\n",
    "            for j in candidates:\n",
    "                high_row = high.iloc[j]\n",
    "                if low_row.geometry.distance(high_row.geometry) <= proximity_threshold:\n",
    "                    valid_low.add(i)\n",
    "                    valid_high.add(high.index[j])\n",
    "        \n",
    "        filtered_low = low.loc[valid_low].copy()\n",
    "        filtered_high = high.loc[valid_high].copy()\n",
    "        \n",
    "        final_all_counties = pd.concat([filtered_low, filtered_high])\n",
    "        final_all_counties[\"dma_name2\"] = dma_name  # Explicitly assign current DMA\n",
    "        final_all_counties2 = pd.concat([final_all_counties2, final_all_counties])\n",
    "\n",
    "\n",
    "        #filtered_low = low.loc[~low.index.isin(final_matched.index)].copy()\n",
    "        #filtered_high = high.loc[~high.index.isin(final_matched.index)].copy()\n",
    "        filtered_low[\"centroid\"] = filtered_low.geometry.centroid\n",
    "        filtered_high[\"centroid\"] = filtered_high.geometry.centroid\n",
    "        \n",
    "        \n",
    "        # --- Begin Modified Bundle Logic ---               \n",
    "\n",
    "        # Step 1: Match low counties with unique DMA to closest high counties\n",
    "\n",
    "        # Compute value counts for DMA names among low overlap counties\n",
    "        dma_counts_low = filtered_low[\"dma_name\"].value_counts()\n",
    "\n",
    "        # Identify low counties with unique DMA name\n",
    "        unique_dma_lows = filtered_low[filtered_low[\"dma_name\"].isin(dma_counts_low[dma_counts_low == 1].index)].copy()\n",
    "\n",
    "        # Prepare tracking\n",
    "        mutual_pairs = []\n",
    "        matched_high = set()\n",
    "        matched_low = set()\n",
    "\n",
    "        # Match each unique DMA low county to its closest unmatched high county\n",
    "        for l_idx, l_row in unique_dma_lows.iterrows():\n",
    "            if pd.isnull(l_row[\"dma_name\"]):\n",
    "                continue  # skip if DMA not assigned\n",
    "\n",
    "            # Only consider unmatched high counties\n",
    "            available_high = filtered_high.loc[~filtered_high.index.isin(matched_high)].copy()\n",
    "            \n",
    "            # Restrict to those that are border-adjacent (touching)\n",
    "            touching_high = available_high[available_high.geometry.touches(l_row.geometry)].copy()\n",
    "\n",
    "\n",
    "            if not touching_high.empty:\n",
    "                touching_high[\"dist\"] = touching_high[\"centroid\"].apply(lambda c: c.distance(l_row[\"centroid\"]))\n",
    "                closest_h_idx = touching_high[\"dist\"].idxmin()\n",
    "\n",
    "\n",
    "                # Record match\n",
    "                mutual_pairs.append((closest_h_idx, l_idx))\n",
    "                matched_high.add(closest_h_idx)\n",
    "                matched_low.add(l_idx)\n",
    "\n",
    "        \n",
    "        # Step 2: Re-match unmatched using same logic, ensuring border adjacency\n",
    "        unmatched_high = filtered_high.loc[~filtered_high.index.isin(matched_high)].copy()\n",
    "        unmatched_low = filtered_low.loc[~filtered_low.index.isin(matched_low)].copy()\n",
    "\n",
    "        high_to_low = {}\n",
    "        low_to_high = {}\n",
    "\n",
    "        # Only consider unmatched pairs that touch\n",
    "        for h_idx, h_row in unmatched_high.iterrows():\n",
    "            touching_lows = unmatched_low[unmatched_low.geometry.touches(h_row.geometry)].copy()\n",
    "            if touching_lows.empty:\n",
    "                continue\n",
    "            touching_lows[\"dist\"] = touching_lows[\"centroid\"].apply(lambda c: c.distance(h_row[\"centroid\"]))\n",
    "            closest_l_idx = touching_lows[\"dist\"].idxmin()\n",
    "            high_to_low[h_idx] = closest_l_idx\n",
    "\n",
    "        for l_idx, l_row in unmatched_low.iterrows():\n",
    "            touching_highs = unmatched_high[unmatched_high.geometry.touches(l_row.geometry)].copy()\n",
    "            if touching_highs.empty:\n",
    "                continue\n",
    "            touching_highs[\"dist\"] = touching_highs[\"centroid\"].apply(lambda c: c.distance(l_row[\"centroid\"]))\n",
    "            closest_h_idx = touching_highs[\"dist\"].idxmin()\n",
    "            low_to_high[l_idx] = closest_h_idx\n",
    "\n",
    "        # Add second-round mutual matches\n",
    "        for h_idx, l_idx in high_to_low.items():\n",
    "            if low_to_high.get(l_idx) == h_idx:\n",
    "                if h_idx not in matched_high and l_idx not in matched_low:\n",
    "                    mutual_pairs.append((h_idx, l_idx))\n",
    "                    matched_high.add(h_idx)\n",
    "                    matched_low.add(l_idx)\n",
    "\n",
    "\n",
    "        # Step 3: Re-match remaining unmatched using same mutual closest logic with adjacency\n",
    "\n",
    "        still_unmatched_high = filtered_high.loc[~filtered_high.index.isin(matched_high)].copy()\n",
    "        still_unmatched_low = filtered_low.loc[~filtered_low.index.isin(matched_low)].copy()\n",
    "\n",
    "        high_to_low = {}\n",
    "        low_to_high = {}\n",
    "\n",
    "        for h_idx, h_row in still_unmatched_high.iterrows():\n",
    "            touching_lows = still_unmatched_low[still_unmatched_low.geometry.touches(h_row.geometry)].copy()\n",
    "            if touching_lows.empty:\n",
    "                continue\n",
    "            touching_lows[\"dist\"] = touching_lows[\"centroid\"].apply(lambda c: c.distance(h_row[\"centroid\"]))\n",
    "            closest_l_idx = touching_lows[\"dist\"].idxmin()\n",
    "            high_to_low[h_idx] = closest_l_idx\n",
    "\n",
    "        for l_idx, l_row in still_unmatched_low.iterrows():\n",
    "            touching_highs = still_unmatched_high[still_unmatched_high.geometry.touches(l_row.geometry)].copy()\n",
    "            if touching_highs.empty:\n",
    "                continue\n",
    "            touching_highs[\"dist\"] = touching_highs[\"centroid\"].apply(lambda c: c.distance(l_row[\"centroid\"]))\n",
    "            closest_h_idx = touching_highs[\"dist\"].idxmin()\n",
    "            low_to_high[l_idx] = closest_h_idx\n",
    "\n",
    "        # Add third-round mutual matches\n",
    "        for h_idx, l_idx in high_to_low.items():\n",
    "            if low_to_high.get(l_idx) == h_idx:\n",
    "                if h_idx not in matched_high and l_idx not in matched_low:\n",
    "                    mutual_pairs.append((h_idx, l_idx))\n",
    "                    matched_high.add(h_idx)\n",
    "                    matched_low.add(l_idx)    \n",
    "\n",
    "        # Convert mutual pairs into bundle structure\n",
    "        bundles = []\n",
    "        county_to_bundle = {}\n",
    "\n",
    "        for h_idx, l_idx in mutual_pairs:\n",
    "            bundle = {h_idx, l_idx}\n",
    "            bundle_idx = len(bundles)\n",
    "            bundles.append(bundle)\n",
    "            county_to_bundle[h_idx] = bundle_idx\n",
    "            county_to_bundle[l_idx] = bundle_idx\n",
    "    \n",
    "    \n",
    "        # Compute centroid of each bundle as centroid of the union of its counties' geometries\n",
    "        bundle_centroids = []\n",
    "        for bundle in bundles:\n",
    "            geoms = band_counties.loc[list(bundle)].geometry\n",
    "            # Sometimes union can be MultiPolygon, so use unary_union\n",
    "            union_geom = geoms.unary_union\n",
    "            centroid = union_geom.centroid\n",
    "            bundle_centroids.append(centroid)\n",
    "    \n",
    "        # Step 4: Match remaining unmatched counties by DMA & nearest bundle\n",
    "        still_unmatched = filtered_low.loc[~filtered_low.index.isin(matched_low)].copy()\n",
    "        still_unmatched = pd.concat([\n",
    "            still_unmatched,\n",
    "            filtered_high.loc[~filtered_high.index.isin(matched_high)].copy()\n",
    "        ])\n",
    "\n",
    "        # Loop through unmatched counties\n",
    "        for idx, row in still_unmatched.iterrows():\n",
    "            county_dma_name = band_counties.loc[idx, \"dma_name\"]\n",
    "            if pd.isnull(county_dma_name):\n",
    "                continue  # skip if county has no DMA assigned\n",
    "\n",
    "            # Find bundles that include at least one county with the same DMA\n",
    "            candidate_bundle_idxs = []\n",
    "            for b_idx, bundle in enumerate(bundles):\n",
    "                bundle_dma_names = band_counties.loc[list(bundle), \"dma_name\"].dropna().unique()\n",
    "                if county_dma_name in bundle_dma_names:\n",
    "                    candidate_bundle_idxs.append(b_idx)\n",
    "\n",
    "            if not candidate_bundle_idxs:\n",
    "                continue  # no compatible bundle found\n",
    "\n",
    "            # Find the closest such bundle\n",
    "            county_centroid = row[\"centroid\"]\n",
    "            closest_bundle_idx = None\n",
    "            min_distance = float(\"inf\")\n",
    "\n",
    "            for b_idx in candidate_bundle_idxs:\n",
    "                bundle_centroid = bundle_centroids[b_idx]\n",
    "                dist = county_centroid.distance(bundle_centroid)\n",
    "                if dist < min_distance:\n",
    "                    min_distance = dist\n",
    "                    closest_bundle_idx = b_idx\n",
    "\n",
    "            # Assign county to the closest bundle\n",
    "            if closest_bundle_idx is not None:\n",
    "                bundles[closest_bundle_idx].add(idx)\n",
    "                county_to_bundle[idx] = closest_bundle_idx\n",
    "\n",
    "                # Recompute bundle centroid\n",
    "                bundle_geom = band_counties.loc[list(bundles[closest_bundle_idx])].geometry.unary_union\n",
    "                bundle_centroids[closest_bundle_idx] = bundle_geom.centroid    \n",
    "\n",
    "        # --- New addition: Check if any unmatched counties remain ---\n",
    "        all_matched_indices = set(county_to_bundle.keys())\n",
    "        all_candidate_indices = set(filtered_low.index).union(set(filtered_high.index))\n",
    "        still_remaining = sorted(all_candidate_indices - all_matched_indices)\n",
    "        \n",
    "        final_matched2 = band_counties.loc[band_counties.index.isin(all_matched_indices)].copy()\n",
    "        final_matched = pd.concat([final_matched, final_matched2])\n",
    "\n",
    "\n",
    "        #if still_remaining:\n",
    "        #   print(\"Counties still unmatched after Step 4:\")\n",
    "        #  print(band_counties.loc[still_remaining][[\"dma_name\"]])\n",
    "        #else:\n",
    "        #    print(\"All counties matched successfully by end of Step 4.\") \n",
    "        \n",
    "        # Step 5: Merge bundles that share the same set of two DMAs\n",
    "\n",
    "        # Step 5.1: Build a mapping from bundle index to DMA set\n",
    "        bundle_dma_sets = {}\n",
    "        for idx, bundle in enumerate(bundles):\n",
    "            dma_set = set(band_counties.loc[list(bundle), \"dma_name\"])\n",
    "            # Only consider bundles that consist of exactly two unique DMAs\n",
    "            if len(dma_set) == 2:\n",
    "                bundle_dma_sets[idx] = frozenset(dma_set)\n",
    "\n",
    "        # Step 5.2: Group bundles by identical DMA sets\n",
    "        dma_pair_to_bundle_idxs = defaultdict(list)\n",
    "        for bundle_idx, dma_pair in bundle_dma_sets.items():\n",
    "            dma_pair_to_bundle_idxs[dma_pair].append(bundle_idx)\n",
    "\n",
    "        # Step 5.3: Create new merged bundles\n",
    "        merged_bundles = []\n",
    "        new_county_to_bundle = {}\n",
    "        seen_bundle_idxs = set()\n",
    "\n",
    "        for dma_pair, bundle_idxs in dma_pair_to_bundle_idxs.items():\n",
    "            # Merge all bundles with same DMA pair\n",
    "            merged_county_set = set()\n",
    "            for b_idx in bundle_idxs:\n",
    "                merged_county_set.update(bundles[b_idx])\n",
    "                seen_bundle_idxs.add(b_idx)\n",
    "    \n",
    "            merged_bundles.append(merged_county_set)\n",
    "            new_bundle_idx = len(merged_bundles) - 1\n",
    "            for c_idx in merged_county_set:\n",
    "                new_county_to_bundle[c_idx] = new_bundle_idx\n",
    "\n",
    "        # Step 5.4: Add remaining unmerged bundles\n",
    "        for i, bundle in enumerate(bundles):\n",
    "            if i in seen_bundle_idxs:\n",
    "                continue  # Already merged\n",
    "            merged_bundles.append(bundle)\n",
    "            new_bundle_idx = len(merged_bundles) - 1\n",
    "            for c_idx in bundle:\n",
    "                new_county_to_bundle[c_idx] = new_bundle_idx\n",
    "\n",
    "        # Replace original bundle structures with merged ones\n",
    "        bundles = merged_bundles\n",
    "        #county_to_bundle = new_county_to_bundle\n",
    "                \n",
    "        # --- End Modified Bundle Logic ---\n",
    "        \n",
    "        \n",
    "        zip_flags = zip_flags_df.set_index(\"zipcode\")[[\"has0\", \"has1\", \"has2\"]]\n",
    "        zip_gdf[\"zipcode\"] = zip_gdf[\"ZCTA5CE20\"].astype(str).str.zfill(5)\n",
    "        zip_gdf = zip_gdf.merge(zip_flags, how=\"left\", left_on=\"zipcode\", right_index=True)\n",
    "\n",
    "        final_counties = pd.concat([filtered_low, filtered_high]).drop_duplicates().to_crs(\"EPSG:4269\")\n",
    "        if final_counties.empty:\n",
    "            print(f\"Skipping DMA {dma_name} due to empty final counties.\")\n",
    "            continue\n",
    "        final_union = final_counties.unary_union\n",
    "        zip_gdf = zip_gdf[zip_gdf.geometry.intersects(final_union)]\n",
    "\n",
    "        # --- Store results for both original and merged bundle assignments ---\n",
    "\n",
    "        # Assign both original (pre-merge) and final (post-merge) bundle IDs\n",
    "        band_counties[\"bundle_id_premerge\"] = band_counties.index.map(county_to_bundle)\n",
    "        band_counties[\"bundle_id_final\"] = band_counties.index.map(new_county_to_bundle)\n",
    "\n",
    "        # Before merge (optional but included for export comparison)\n",
    "        bundle_county_gdf_premerge = band_counties[band_counties[\"bundle_id_premerge\"].notna()][\n",
    "            [\"geometry\", \"bundle_id_premerge\", \"pct_overlap\", \"NAME\", \"dma_name\"]\n",
    "        ].copy()\n",
    "        bundle_county_gdf_premerge[\"dma_name2\"] = dma_name  # Explicitly assign current DMA\n",
    "\n",
    "\n",
    "        # After merge (final classification)\n",
    "        bundle_county_gdf_final = band_counties[band_counties[\"bundle_id_final\"].notna()][\n",
    "            [\"geometry\", \"bundle_id_final\", \"pct_overlap\", \"NAME\", \"dma_name\"]\n",
    "        ].copy()\n",
    "        bundle_county_gdf_final[\"dma_name2\"] = dma_name  # Explicitly assign current DMA\n",
    "\n",
    "\n",
    "\n",
    "        zip_proj = zip_gdf.to_crs(projected_crs)\n",
    "        bundle_county_proj_premerge = bundle_county_gdf_premerge.to_crs(projected_crs)\n",
    "        bundle_county_proj_final = bundle_county_gdf_final.to_crs(projected_crs)\n",
    "        \n",
    "        premerge_matched_counties_F = pd.concat([premerge_matched_counties_F, premerge_matched_counties_F])\n",
    "        final_matched_counties_F = pd.concat([final_matched_counties_F, bundle_county_proj_final])\n",
    "\n",
    "\n",
    "        zip_bundle_records = []\n",
    "\n",
    "        for i, zip_row in zip_proj.iterrows():\n",
    "            zip_geom = zip_row.geometry\n",
    "\n",
    "            # --- Premerge assignment ---\n",
    "            overlaps_pre = []\n",
    "            for j, county_row in bundle_county_proj_premerge.iterrows():\n",
    "                intersection = zip_geom.intersection(county_row.geometry)\n",
    "                if not intersection.is_empty:\n",
    "                    overlaps_pre.append({\n",
    "                        \"bundle_id_premerge\": int(county_row[\"bundle_id_premerge\"]) + 1,\n",
    "                        \"pct_overlap_premerge\": county_row[\"pct_overlap\"],\n",
    "                        \"overlap_area\": intersection.area\n",
    "                    })\n",
    "\n",
    "            if overlaps_pre:\n",
    "                overlaps_pre.sort(key=lambda x: x[\"overlap_area\"], reverse=True)\n",
    "                assigned_bundle_pre = overlaps_pre[0][\"bundle_id_premerge\"]\n",
    "                assigned_class_pre = \"<15\" if overlaps_pre[0][\"pct_overlap_premerge\"] < 0.15 else \">80\"\n",
    "            else:\n",
    "                assigned_bundle_pre = None\n",
    "                assigned_class_pre = None\n",
    "\n",
    "            # --- Final assignment ---\n",
    "            overlaps_final = []\n",
    "            for j, county_row in bundle_county_proj_final.iterrows():\n",
    "                intersection = zip_geom.intersection(county_row.geometry)\n",
    "                if not intersection.is_empty:\n",
    "                    overlaps_final.append({\n",
    "                        \"bundle_id_final\": int(county_row[\"bundle_id_final\"]) + 1,\n",
    "                        \"pct_overlap_final\": county_row[\"pct_overlap\"],\n",
    "                        \"overlap_area\": intersection.area\n",
    "                    })            \n",
    "            \n",
    "            # --- Final assignment ---\n",
    "#            if overlaps_final:\n",
    "#                overlaps_final.sort(key=lambda x: x[\"overlap_area\"], reverse=True)\n",
    "#                assigned_bundle_final = overlaps_final[0][\"bundle_id_final\"]\n",
    "#                assigned_class_final = \"<15\" if overlaps_final[0][\"pct_overlap_final\"] < 0.15 else \">80\"\n",
    "\n",
    "                # Get the corresponding county row again (with full info)\n",
    "#                county_info = county_row[[\"geometry\", \"bundle_id_final\", \"pct_overlap\", \"NAME\", \"dma_name\"]].to_dict()\n",
    "#                county_info[\"dma_name2\"] = dma_name  # Add DMA from loop\n",
    "#            else:\n",
    "#                assigned_bundle_final = None\n",
    "#                assigned_class_final = None\n",
    "#                county_info = {\n",
    "#                    \"geometry\": None,\n",
    "#                    \"bundle_id_final\": None,\n",
    "#                    \"pct_overlap\": None,\n",
    "#                    \"NAME\": None,\n",
    "#                    \"dma_name\": None,\n",
    "#                    \"dma_name2\": dma_name\n",
    "#                }\n",
    "                \n",
    "            # --- Final assignment ---\n",
    "            overlaps_final = []\n",
    "            for j, county_row in bundle_county_proj_final.iterrows():\n",
    "                intersection = zip_geom.intersection(county_row.geometry)\n",
    "                if not intersection.is_empty:\n",
    "                    overlaps_final.append({\n",
    "                        \"bundle_id_final\": int(county_row[\"bundle_id_final\"]) + 1,\n",
    "                        \"pct_overlap_final\": county_row[\"pct_overlap\"],\n",
    "                        \"overlap_area\": intersection.area,\n",
    "                        \"county_info\": county_row  # ✅ Store the full row!\n",
    "                    })\n",
    "\n",
    "            if overlaps_final:\n",
    "                overlaps_final.sort(key=lambda x: x[\"overlap_area\"], reverse=True)\n",
    "                best_match = overlaps_final[0]\n",
    "                assigned_bundle_final = best_match[\"bundle_id_final\"]\n",
    "                assigned_class_final = \"<15\" if best_match[\"pct_overlap_final\"] < 0.15 else \">80\"\n",
    "\n",
    "                county_info = best_match[\"county_info\"][[\"geometry\", \"bundle_id_final\", \"pct_overlap\", \"NAME\", \"dma_name\"]].to_dict()\n",
    "                county_info[\"dma_name2\"] = dma_name  # Add DMA from outer loop\n",
    "            else:\n",
    "                assigned_bundle_final = None\n",
    "                assigned_class_final = None\n",
    "                county_info = {\n",
    "                    \"geometry\": None,\n",
    "                    \"bundle_id_final\": None,\n",
    "                    \"pct_overlap\": None,\n",
    "                    \"NAME\": None,\n",
    "                    \"dma_name\": None,\n",
    "                    \"dma_name2\": dma_name\n",
    "                }\n",
    "\n",
    "\n",
    "            # Combine ZIP info and county info\n",
    "            zip_bundle_records.append({\n",
    "                \"zipcode\": zip_row[\"zipcode\"],\n",
    "                \"bundle_premerge\": assigned_bundle_pre,\n",
    "                \"class_premerge\": assigned_class_pre,\n",
    "                \"bundle_final\": assigned_bundle_final,\n",
    "                \"class_final\": assigned_class_final,\n",
    "                **county_info  # Unpack county info into the dict\n",
    "            })\n",
    "            \n",
    "\n",
    "        assignments_df = pd.DataFrame(zip_bundle_records)\n",
    "        all_assignments.append(assignments_df)\n",
    "        print(f\"Processed DMA: {dma_name}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Skipping DMA {dma_name} due to error: {e}\")\n",
    "\n",
    "# Merge final assignments\n",
    "final_assignments_df = pd.concat(all_assignments, ignore_index=True)\n",
    "zip_flags_df = original_zip_flags_df.copy()\n",
    "zip_flags_df = zip_flags_df.merge(final_assignments_df, on=\"zipcode\", how=\"left\")\n",
    "\n",
    "\n",
    "# Replace only if missing\n",
    "zip_flags_df[\"assigned_bundle_1\"] = zip_flags_df[\"assigned_bundle_1\"].where(\n",
    "    zip_flags_df[\"assigned_bundle_1\"].notna(), zip_flags_df[\"bundle_premerge\"])\n",
    "zip_flags_df[\"assigned_bundle_2\"] = zip_flags_df[\"assigned_bundle_2\"].where(\n",
    "    zip_flags_df[\"assigned_bundle_2\"].notna(), zip_flags_df[\"bundle_final\"])\n",
    "zip_flags_df[\"overlap_class_1\"] = zip_flags_df[\"overlap_class_1\"].where(\n",
    "    zip_flags_df[\"overlap_class_1\"].astype(str).str.strip() != \"\",\n",
    "    zip_flags_df[\"class_premerge\"])\n",
    "zip_flags_df[\"overlap_class_2\"] = zip_flags_df[\"overlap_class_2\"].where(\n",
    "    zip_flags_df[\"overlap_class_2\"].astype(str).str.strip() != \"\",\n",
    "    zip_flags_df[\"class_final\"]\n",
    ")\n",
    "\n",
    "# Drop geometry before exporting\n",
    "export_df = zip_flags_df.drop(columns=[\"geometry\"], errors=\"ignore\")\n",
    "\n",
    "# Save final output\n",
    "#output_path = os.path.join(os.path.dirname(zip_flags_fp), \"zip_duplicates_all_dmas_experiemntal_restricted_to_two.dta\")\n",
    "#export_df.to_stata(output_path, write_index=False)\n",
    "\n",
    "#print(f\"Final ZIP flags saved to: {output_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3664ca-a1e2-48b1-b4c0-99a7ae81d2f4",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "96ee4ed3-ba15-4deb-8fab-31f24cbb0c7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Let's see how many counties remain without any bundle (I called them Phoenix, just because)\n",
    "    \n",
    "phoenix_matches_sorted = final_matched_counties_F.sort_values(by=\"NAME\", ascending=False).reset_index(drop=True).drop(columns=['geometry'], errors=\"ignore\")\n",
    "phoenix_all_sorted = final_all_counties2.sort_values(by=\"NAME\", ascending=False).reset_index(drop=True).drop(columns=['geometry'], errors=\"ignore\")\n",
    "\n",
    "# Columns to exclude from renaming\n",
    "exclude_cols = {\"NAME\", \"dma_name\", \"dma_name2\", \"pct_overlap\"}\n",
    "\n",
    "# Rename columns conditionally\n",
    "phoenix_all_sorted = phoenix_all_sorted.rename(columns={col: f\"{col}_ALL\" for col in phoenix_all_sorted.columns if col not in exclude_cols})\n",
    "phoenix_matches_sorted = phoenix_matches_sorted.rename(columns={col: f\"{col}_MERGED\" for col in phoenix_matches_sorted.columns if col not in exclude_cols})\n",
    "\n",
    "merged_phoenix = pd.merge(phoenix_matches_sorted,phoenix_all_sorted,on=[\"NAME\", \"dma_name\", \"dma_name2\", \"pct_overlap\"],how=\"outer\",indicator=True)\n",
    "\n",
    "merged_phoenix = merged_phoenix.drop(columns=[\"geometry_MATCH\", \"geometry_ALL\", \"geometry\"], errors=\"ignore\")\n",
    "\n",
    "# The following line can be deleted\n",
    "export_df = pd.read_stata(\"/Users/oliverraab/Desktop/CUNY/5_semester/Short_Project3/Analysis/ZIP_code_alternative_solution(contain_pyhon_file_with_DMA_calculation)/zip_duplicates_all_dmas_experiemntal_restricted_to_two.dta\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "e4bdca35-303d-42f4-a5f8-38869171c057",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oliverraab/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3050: DtypeWarning: Columns (72,78,79,159,163,166,167,168,171,175,178,179,180,202,203,205,224,225,226,235,236,237,241,242,243,244,251,284,285,286,287,288,289,292,293,294,295,296,297,300,301) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "# --- Load data ---\n",
    "input_path = \"/Users/oliverraab/Desktop/CUNY/5_semester/Short_Project3/Factbooks/FINAL_DMA_CHANNELS.xlsx\"\n",
    "df_DMA = pd.read_excel(input_path)\n",
    "\n",
    "input2_path = \"/Users/oliverraab/Desktop/CUNY/5_semester/Short_Project3/Analysis/ZIP_code_alternative_solution(contain_pyhon_file_with_DMA_calculation)/FINAL_DATA_MERGED/final_dataset_2.csv\"\n",
    "df_FINAL_DATA = pd.read_csv(input2_path)\n",
    "\n",
    "dma_fp = \"/Users/oliverraab/Desktop/CUNY/5_semester/Short_Project3/Data/DMA_data/nielsen-dma-master/nielsen_dma.json\"\n",
    "zip_shapefile_fp = \"/Users/oliverraab/Desktop/CUNY/5_semester/Short_Project3/Data/ZIP_code_coordinates_new/tl_2020_us_zcta520/tl_2020_us_zcta520.shp\"\n",
    "\n",
    "# --- Load geodata ---\n",
    "dmas = gpd.read_file(dma_fp).to_crs(\"EPSG:4269\")\n",
    "zip_gdf_master = gpd.read_file(zip_shapefile_fp).set_crs(\"EPSG:4269\")\n",
    "\n",
    "# --- Filter ZIPs of interest ---\n",
    "zips_of_interest = export_df[\"zipcode\"].astype(str).unique()\n",
    "zip_gdf_filtered = zip_gdf_master[zip_gdf_master[\"ZCTA5CE20\"].isin(zips_of_interest)].copy()\n",
    "\n",
    "# --- Project to equal-area CRS for accurate area measurement ---\n",
    "dmas_proj = dmas.to_crs(\"EPSG:5070\")              # NAD83 / Conus Albers (square meters)\n",
    "zip_gdf_proj = zip_gdf_filtered.to_crs(\"EPSG:5070\")\n",
    "\n",
    "# --- Compute total ZIP area ---\n",
    "zip_gdf_proj[\"zip_area\"] = zip_gdf_proj.geometry.area\n",
    "\n",
    "# --- Compute intersections ---\n",
    "intersections = gpd.overlay(zip_gdf_proj, dmas_proj, how=\"intersection\")\n",
    "\n",
    "# --- Compute overlap area ---\n",
    "intersections[\"overlap_area\"] = intersections.geometry.area\n",
    "\n",
    "# --- Compute percent overlap ---\n",
    "intersections[\"pct_overlap\"] = (intersections[\"overlap_area\"] / intersections[\"zip_area\"]) * 100\n",
    "\n",
    "# --- Sort by ZIP and overlap area ---\n",
    "intersections_sorted = intersections.sort_values(by=[\"ZCTA5CE20\", \"overlap_area\"], ascending=[True, False])\n",
    "\n",
    "n_df = intersections_sorted\n",
    "zip_to_dma2 = n_df[[\"ZCTA5CE20\", \"dma1\", \"overlap_area\", \"pct_overlap\"]].rename(columns={\"ZCTA5CE20\": \"zipcode\", \"dma1\": \"DMA_NEW\", \"overlap_area\" : \"zip_overlap\", \"pct_overlap\" : \"zip_pct_overlap\"})\n",
    "\n",
    "zip_to_dma2 = zip_to_dma2.rename(columns={\"DMA_NEW\": \"dma_name\"})\n",
    "\n",
    "# --- Merge with your main dataset ---\n",
    "export_df3 = export_df.merge(zip_to_dma2, on=[\"zipcode\", \"dma_name\"], how=\"left\")\n",
    "\n",
    "export_df4 = export_df3[export_df3['assigned_bundle_1'].notna()]\n",
    "\n",
    "export_df5 = export_df4[~(export_df4['zip_pct_overlap'] < 30)]\n",
    "export_df5 = export_df5[export_df5['zip_pct_overlap'].notna()] #to try only\n",
    "\n",
    "export_df_in_case = export_df\n",
    "\n",
    "export_df = export_df5\n",
    "\n",
    "#export_df = export_df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "dc3f14c1-5aca-41f6-b7c1-8f034ff22979",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Here I am merging the dataset to see if those counties that are not divided into bundles affect any ZIP code\n",
    "\n",
    "    # Step 1: Sort \n",
    "export_df = export_df.sort_values(by=[\"NAME\", \"pct_overlap\", \"dma_name\", \"dma_name2\"]).reset_index(drop=True)\n",
    "    # Step 2: Merge: I need to merge ZIPs with border counties to see if there are any \n",
    "merged = pd.merge(export_df, merged_phoenix, on=[\"NAME\", \"pct_overlap\", \"dma_name\", \"dma_name2\"], how=\"left\", indicator=\"_merge2\")\n",
    "    # Step 3: Drop rows that exist only in merged_phoenix (\"_merge2\" == \"right_only\"): meaning, I am keeping only those ZIPs, that are in border counties\n",
    "merged = merged[merged[\"_merge2\"] != \"right_only\"]\n",
    "    # Step 4: BE READY TO CHANGE THIS ASSUMPTION IN THE NEXT CODE VARIATION: I assume that I need only those ZIP codes that are in all periods and are in fact in border counties. \n",
    "    # I can be less restrictive with the ZIP codes. \n",
    "merged_restricted = merged[(merged[\"_merge2\"] == 'both')&(merged[\"has0\"] == 1) & (merged[\"has1\"] == 1) & (merged[\"has2\"] == 1)]\n",
    "#Check the unmerged. If you have only both, continue with data cleaning\n",
    "#merged_restricted[\"_merge\"].unique() \n",
    "    # Step 5: I assign number to each category to find the unique bundles accross all DMAs\n",
    "merged_restricted = merged_restricted.copy()\n",
    "merged_restricted[\"dma_name2_num\"] = pd.factorize(merged_restricted[\"dma_name2\"])[0]\n",
    "\n",
    "merged_restricted[\"unique_bundle\"] = (merged_restricted[\"bundle_final\"].fillna(-1).astype(int).astype(str) + merged_restricted[\"dma_name2_num\"].astype(int).astype(str)).astype(int)\n",
    "\n",
    "sorted_unique_values = sorted(merged_restricted[\"unique_bundle\"].unique())\n",
    "unique_bundle_map = {val: i+1 for i, val in enumerate(sorted_unique_values)}\n",
    "merged_restricted[\"unique_bundle_sorted\"] = merged_restricted[\"unique_bundle\"].map(unique_bundle_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "de64cb2a-1185-4796-948d-2efc96525c71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Find if the ZIP is part only of one side of the bundle. It can happend that all ZIPs in the bundle are <80, but not >15 or vice versa. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "merged_restricted_first = merged_restricted.drop_duplicates(subset=[\"unique_bundle_sorted\", \"class_final\"])\n",
    "\n",
    "# Count occurrences of each unique_bundle_sorted\n",
    "counts = merged_restricted_first[\"unique_bundle_sorted\"].value_counts()\n",
    "\n",
    "# Identify values that occur exactly once\n",
    "unique_bundles = counts[counts == 1].index\n",
    "\n",
    "# Keep only rows with unique_bundle_sorted that occur exactly once\n",
    "merged_restricted_first_filtered = merged_restricted_first[merged_restricted_first[\"unique_bundle_sorted\"].isin(unique_bundles)].copy()\n",
    "merged_restricted_first_filtered = merged_restricted_first_filtered[[\"class_premerge\", \"unique_bundle_sorted\"]].copy()\n",
    "merged_restricted_merged = pd.merge(merged_restricted, merged_restricted_first_filtered, on=[\"class_premerge\", \"unique_bundle_sorted\"], how=\"left\", indicator=\"_merge_flag\")\n",
    "\n",
    "#Here: merge_flag tells if the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "ecd64835-7734-425d-9322-928a76294b2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#len(merged_restricted_first_filtered[\"unique_bundle_sorted\"].unique())\n",
    "#merged_restricted_merged.sort_values(by=[\"unique_bundle\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "ce475cc8-7967-40f4-9cac-77ec406fd028",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Check if bundles are symmetrical: meaning, that one more than one bundle contains the same counties and ZIP codes\n",
    "\n",
    "\n",
    "\n",
    "merged_restricted_merged[\"new_var\"] = merged_restricted_merged[\"zipcode\"] + \" \" + merged_restricted_merged[\"NAME\"] + \" \" + merged_restricted_merged[\"dma_name\"]\n",
    "\n",
    "# Step 1: Group by 'unique_bundle_sorted' and collect the set of 'new_var' values\n",
    "bundle_to_newvar = merged_restricted_merged.groupby(\"unique_bundle_sorted\")[\"new_var\"].apply(lambda x: frozenset(x.unique()))\n",
    "\n",
    "# Step 2: Reverse the mapping: which bundles share the same set of new_var values\n",
    "newvar_to_bundles = bundle_to_newvar.reset_index().groupby(\"new_var\")[\"unique_bundle_sorted\"].apply(list)\n",
    "\n",
    "# Step 3: Mark bundles that share the same new_var set with another bundle\n",
    "shared_bundle_flags = bundle_to_newvar.map(lambda x: len(newvar_to_bundles[x]) > 1)\n",
    "\n",
    "# Step 4: Merge this info back into the original dataframe\n",
    "merged_restricted_merged = merged_restricted_merged.merge(\n",
    "    shared_bundle_flags.rename(\"duplicate_newvar_bundle_flag\"),\n",
    "    left_on=\"unique_bundle_sorted\", right_index=True\n",
    ")\n",
    "\n",
    "# Optional: Convert boolean to integer (1 if shared, 0 if not)\n",
    "merged_restricted[\"duplicate_newvar_bundle_flag\"] = merged_restricted_merged[\"duplicate_newvar_bundle_flag\"].astype(int)\n",
    "#merged_restricted.sort_values(by=[\"unique_bundle_sorted\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "f4b7c0d8-5577-4b12-bff6-3460cda58a7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_restricted_merged[\"duplicates\"] = merged_restricted_merged.duplicated(\"zipcode\", keep=False).astype(int)\n",
    "#merged_restricted.sort_values(by=[\"zipcode\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#output_path = os.path.join(os.path.dirname(zip_flags_fp), \"merged_restricted.dta\")\n",
    "#merged_restricted_merged.to_stata(output_path, write_index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "637ebdd3-aff4-47db-9644-eb67f007b866",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "464"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here I eliminate those observations that are symetrical and already in the bundle: I eliminated from 738 to 570 observations (with goal to get 314 unique zipcodes)\n",
    "\n",
    "#STEP 1: I eliminate all the lone observations that are duplicated by being assigned to symmetrical bundle\n",
    "\n",
    "\n",
    "\n",
    "# Assign duplication order: 0 for first, 1 for second, etc.\n",
    "merged_restricted_merged[\"duplicates2\"] = merged_restricted_merged.groupby(\"zipcode\").cumcount()\n",
    "\n",
    "merged_restricted_merged2 = merged_restricted_merged.drop(\n",
    "    merged_restricted_merged[\n",
    "        (merged_restricted_merged[\"duplicate_newvar_bundle_flag\"] == 1) &\n",
    "        (merged_restricted_merged[\"_merge_flag\"] == \"both\") &\n",
    "        (merged_restricted_merged[\"duplicates2\"] > 0)\n",
    "    ].index\n",
    ")\n",
    "\n",
    "\n",
    "len(merged_restricted_merged2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "d5a83a6d-c230-4621-9703-475d43176069",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "424"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#STEP 2: I eliminate all the observations that are already assigned to full bundle that are duplicated by being assigned to symmetrical bundle: I eliminate up to 508 (goal is 314)\n",
    "\n",
    "merged_restricted_merged2 = merged_restricted_merged2.drop(columns=[\"duplicate_newvar_bundle_flag\", \"duplicates\", \"duplicates2\"], errors=\"ignore\")\n",
    "\n",
    "# Step 1: Group by 'unique_bundle_sorted' and collect the set of 'new_var' values\n",
    "bundle_to_newvar = merged_restricted_merged2.groupby(\"unique_bundle_sorted\")[\"new_var\"].apply(lambda x: frozenset(x.unique()))\n",
    "\n",
    "# Step 2: Reverse the mapping: which bundles share the same set of new_var values\n",
    "newvar_to_bundles = bundle_to_newvar.reset_index().groupby(\"new_var\")[\"unique_bundle_sorted\"].apply(list)\n",
    "\n",
    "# Step 3: Mark bundles that share the same new_var set with another bundle\n",
    "shared_bundle_flags = bundle_to_newvar.map(lambda x: len(newvar_to_bundles[x]) > 1)\n",
    "\n",
    "# Step 4: Merge this info back into the original dataframe\n",
    "merged_restricted_merged2 = merged_restricted_merged2.merge(\n",
    "    shared_bundle_flags.rename(\"duplicate_newvar_bundle_flag\"),\n",
    "    left_on=\"unique_bundle_sorted\", right_index=True\n",
    ")\n",
    "\n",
    "# Optional: Convert boolean to integer (1 if shared, 0 if not)\n",
    "#merged_restricted[\"duplicate_newvar_bundle_flag\"] = merged_restricted_merged[\"duplicate_newvar_bundle_flag\"].astype(int)\n",
    "#merged_restricted.sort_values(by=[\"unique_bundle_sorted\"])\n",
    "\n",
    "merged_restricted_merged2[\"duplicates\"] = merged_restricted_merged2.duplicated(\"zipcode\", keep=False).astype(int)\n",
    "#merged_restricted.sort_values(by=[\"zipcode\"])\n",
    "# Assign duplication order: 0 for first, 1 for second, etc.\n",
    "merged_restricted_merged2[\"duplicates2\"] = merged_restricted_merged.groupby(\"zipcode\").cumcount()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#output_path = os.path.join(os.path.dirname(zip_flags_fp), \"merged_restricted_merged2.dta\")\n",
    "#merged_restricted_merged2.to_stata(output_path, write_index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "merged_restricted_merged3 = merged_restricted_merged2.drop(\n",
    "    merged_restricted_merged2[\n",
    "        (merged_restricted_merged2[\"duplicate_newvar_bundle_flag\"] == 1) &\n",
    "        (merged_restricted_merged2[\"_merge_flag\"] == \"left_only\") &\n",
    "        (merged_restricted_merged2[\"duplicates2\"] > 0)\n",
    "    ].index\n",
    ")\n",
    "\n",
    "len(merged_restricted_merged3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "9e81032a-e11e-42d4-ae3a-b18dfed1d883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "275"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(merged_restricted_merged3[\"zipcode\"].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "1feee32a-a35f-4aea-a727-4ed984cb59be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# STEP3: I have constructed an alghoritm to eliminate those ZIPs that are duplicated and are not symmetrical\n",
    "# The logic is to eliminate ZIP from the bundle that has surpluss of ZIPs\n",
    "\n",
    "\n",
    "# Here I am looking for correct way how to clean duplicated from the data \n",
    "\n",
    "merged_restricted_unmatched = merged_restricted_merged3[merged_restricted_merged3[\"_merge_flag\"] != \"both\"].copy()\n",
    "counts = merged_restricted_unmatched.groupby([\"unique_bundle_sorted\", \"class_final\"])[\"zipcode\"].nunique().reset_index(name=\"zip_count\")\n",
    "dup_zip_df = merged_restricted_unmatched[merged_restricted_unmatched.duplicated(\"zipcode\", keep=False)].copy()\n",
    "dup_zip_df = dup_zip_df.merge(counts, on=[\"unique_bundle_sorted\", \"class_final\"], how=\"left\")\n",
    "\n",
    "# Initialize the new column with 0\n",
    "dup_zip_df['is_max_zip_count'] = 0\n",
    "\n",
    "# Loop through each unique ZIP code\n",
    "for zip_code in dup_zip_df['zipcode'].unique():\n",
    "    subset = dup_zip_df[dup_zip_df['zipcode'] == zip_code]\n",
    "    max_zip_count = subset['zip_count'].max()\n",
    "    \n",
    "    # Check if the max occurs only once (no tie)\n",
    "    if (subset['zip_count'] == max_zip_count).sum() == 1:\n",
    "        # Find the index of the unique max and set it to 1\n",
    "        idx = subset[subset['zip_count'] == max_zip_count].index[0]\n",
    "        dup_zip_df.loc[idx, 'is_max_zip_count'] = 1\n",
    "\n",
    "dup_zip_df = dup_zip_df[['zipcode', 'class_final', 'unique_bundle_sorted', 'zip_count', 'is_max_zip_count']]\n",
    "merged_restricted_unmatched = merged_restricted_unmatched.drop(columns=['is_max_zip_count', 'is_max_zip_count_x', 'is_max_zip_count_y'], errors=\"ignore\")\n",
    "merged_restricted_unmatched = merged_restricted_unmatched.merge(dup_zip_df, on=['zipcode', 'class_final', 'unique_bundle_sorted'], how=\"left\")\n",
    "\n",
    "\n",
    "merged_restricted_unmatched2 = merged_restricted_unmatched[merged_restricted_unmatched['is_max_zip_count'] != 1]\n",
    "counts2 = merged_restricted_unmatched2.groupby([\"unique_bundle_sorted\", \"class_final\"])[\"zipcode\"].nunique().reset_index(name=\"zip_count2\")\n",
    "dup_zip_df2 = merged_restricted_unmatched2[merged_restricted_unmatched2.duplicated(\"zipcode\", keep=False)].copy()\n",
    "dup_zip_df2 = dup_zip_df2.drop(columns=['zip_count2', 'zip_count2_x', 'zip_count2_y'], errors=\"ignore\")\n",
    "dup_zip_df2 = dup_zip_df2.merge(counts2, on=[\"unique_bundle_sorted\", \"class_final\"], how=\"left\")\n",
    "\n",
    "# Initialize the new column with 0\n",
    "dup_zip_df2['is_max_zip_count2'] = 0\n",
    "\n",
    "# Loop through each unique ZIP code\n",
    "for zip_code in dup_zip_df2['zipcode'].unique():\n",
    "    subset = dup_zip_df2[dup_zip_df2['zipcode'] == zip_code]\n",
    "    max_zip_count2 = subset['zip_count2'].max()\n",
    "    \n",
    "    # Check if the max occurs only once (no tie)\n",
    "    if (subset['zip_count2'] == max_zip_count2).sum() == 1:\n",
    "        # Find the index of the unique max and set it to 1\n",
    "        idx = subset[subset['zip_count2'] == max_zip_count2].index[0]\n",
    "        dup_zip_df2.loc[idx, 'is_max_zip_count2'] = 1\n",
    "\n",
    "dup_zip_df2 = dup_zip_df2[['zipcode', 'class_final', 'unique_bundle_sorted', 'zip_count2', 'is_max_zip_count2']]\n",
    "merged_restricted_unmatched = merged_restricted_unmatched.drop(columns=['is_max_zip_count2_x', 'is_max_zip_count2_y', 'is_max_zip_count2'], errors=\"ignore\")\n",
    "merged_restricted_unmatched = merged_restricted_unmatched.merge(dup_zip_df2, on=['zipcode', 'class_final', 'unique_bundle_sorted'], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "8bb83570-3c23-4b28-8070-3fa0a4de2c4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "166"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(merged_restricted_unmatched[(merged_restricted_unmatched[\"is_max_zip_count\"] != 1) & (merged_restricted_unmatched[\"is_max_zip_count2\"] != 1)].sort_values(by=[\"zipcode\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "5c7ecfb5-4069-4489-8442-5e59ac099768",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_duplicates = merged_restricted_unmatched[(merged_restricted_unmatched[\"is_max_zip_count\"] != 1) & (merged_restricted_unmatched[\"is_max_zip_count2\"] != 1)].sort_values(by=[\"zipcode\"])\n",
    "\n",
    "len(new_duplicates[\"zipcode\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "a452deae-e3aa-4091-9bc7-66ed81fedac5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# STEP4: The last elimination of the duplicates\n",
    "\n",
    "\n",
    "\n",
    "new_duplicates[\"duplicates\"] = new_duplicates.duplicated(\"zipcode\", keep=False).astype(int)\n",
    "sorted2=new_duplicates.sort_values(by=[\"unique_bundle_sorted\", \"class_final\"])\n",
    "\n",
    "#sorted2[[\"zipcode\", \"bundle_final\", \"class_final\", \"unique_bundle_sorted\", \"duplicates\"]]\n",
    "\n",
    "# Step 1: Group by unique_bundle_sorted and check if all duplicates == 1\n",
    "group_all_duplicates = sorted2.groupby(\"unique_bundle_sorted\")[\"duplicates\"].transform(lambda x: int((x == 1).all()))\n",
    "\n",
    "# Step 2: Assign to a new column\n",
    "sorted2[\"all_duplicates_in_group\"] = group_all_duplicates\n",
    "#sorted2[[\"zipcode\", \"bundle_final\", \"class_final\", \"unique_bundle_sorted\", \"duplicates\", \"all_duplicates_in_group\"]]\n",
    "#sorted2[[\"zipcode\", \"bundle_final\", \"class_final\", \"unique_bundle_sorted\", \"duplicates\", \"all_duplicates_in_group\"]].sort_values(by=\"zipcode\")\n",
    "sorted2[\"zipcode\"].nunique()\n",
    "\n",
    "sorted2[\"drop_stage_one\"] = ((sorted2[\"duplicates\"] == 1) & (sorted2[\"all_duplicates_in_group\"] == 1)).astype(int)\n",
    "#sorted2\n",
    "\n",
    "# Start with the original DataFrame\n",
    "df = new_duplicates.copy()\n",
    "\n",
    "# Initialize a variable to control the loop\n",
    "rows_dropped = True\n",
    "\n",
    "# Loop until no ZIPs are dropped\n",
    "while rows_dropped:\n",
    "    # Step 1: Mark duplicate ZIP codes\n",
    "    df[\"duplicates\"] = df.duplicated(\"zipcode\", keep=False).astype(int)\n",
    "    df = df.sort_values(by=[\"unique_bundle_sorted\", \"class_final\"])\n",
    "    \n",
    "    # Step 2: Check if all ZIPs in group are duplicates\n",
    "    df[\"all_duplicates_in_group\"] = df.groupby(\"unique_bundle_sorted\")[\"duplicates\"].transform(lambda x: int((x == 1).all()))\n",
    "    \n",
    "    # Step 3: Create drop_stage_one flag\n",
    "    df[\"drop_stage_one\"] = ((df[\"duplicates\"] == 1) & (df[\"all_duplicates_in_group\"] == 1)).astype(int)\n",
    "\n",
    "    # Step 4: Identify the first bundle to drop (if any)\n",
    "    bundles_to_drop = df.loc[df[\"drop_stage_one\"] == 1, \"unique_bundle_sorted\"].unique()\n",
    "    \n",
    "    if len(bundles_to_drop) > 0:\n",
    "        # Only drop the *first* bundle (lowest-numbered one)\n",
    "        first_bundle = bundles_to_drop.min()\n",
    "        rows_before = len(df)\n",
    "        df = df[df[\"unique_bundle_sorted\"] != first_bundle].copy()\n",
    "        rows_after = len(df)\n",
    "        rows_dropped = rows_before != rows_after  # Update condition\n",
    "    else:\n",
    "        rows_dropped = False  # No rows to drop → end loop\n",
    "\n",
    "# Final result in `df`\n",
    "sorted3 = df.copy()\n",
    "\n",
    "sorted4 = sorted3[sorted3[\"duplicates\"] == 1][[\"zipcode\", \"unique_bundle_sorted\", \"class_final\"]]\n",
    "\n",
    "#sorted4.sort_values(by=[\"zipcode\"])\n",
    "\n",
    "sorted5 = sorted3.merge(sorted4, on=['zipcode', 'class_final', 'unique_bundle_sorted'], indicator=\"_merge_last\", how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "b61d0787-4e60-4746-9ec8-646901e05796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "164"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sorted5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "9b8bf921-8026-4b5d-940f-49b692c4c6d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sorted5['zipcode'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "5305bece-7d9d-4023-8fa6-2f1ada94cc9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_path = os.path.join(os.path.dirname(zip_flags_fp), \"sorted5_UPDATED_30+strict.dta\")\n",
    "sorted5.to_stata(output_path, write_index=False)\n",
    "\n",
    "\n",
    "#\n",
    "#\n",
    "# Sorted5 is the last last cleared stage. I have 213 observations with 200 unique ZIPcodes. \n",
    "#I can randomly delete one of the duplicates, but I would eliminate some important observations (ending up with 187)\n",
    "#\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "5907e518-d246-4a8e-ab38-8036cd45c2e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataset contains 442 rows with ZIPs from sorted5.\n"
     ]
    }
   ],
   "source": [
    "sorted5['zipcode'] = sorted5['zipcode'].astype(str).str.zfill(5)\n",
    "\n",
    "# Step 2: Load the full cleaned dataset from Stata\n",
    "cleaned_data_fp = \"/Users/oliverraab/Desktop/CUNY/5_semester/Short_Project3/Analysis/Merged_Data_Final_Final.dta\"\n",
    "full_df = pd.read_stata(cleaned_data_fp)\n",
    "\n",
    "full_df['zipcode'] = full_df['zipcode'].astype(str).str.zfill(5)\n",
    "\n",
    "# Step 3: Filter full_df to only ZIP codes in sorted5\n",
    "zipcodes_to_keep = sorted5['zipcode'].unique()\n",
    "filtered_df = full_df[full_df['zipcode'].isin(zipcodes_to_keep)].copy()\n",
    "\n",
    "print(f\"Filtered dataset contains {len(filtered_df)} rows with ZIPs from sorted5.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "2edaf796-e917-440f-ba5d-5ab81662db1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#. dis 611/12372 = 0.04938571 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "80148cf8-b940-4d45-9047-d371f712df74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#.updated :  dis 608/12372 = 0.04   if rule of <30 or <50 is used\n",
    "#.updated :  dis 442/12372 = 0.04   if rule of <50 STRICT or <30 STRICT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba8928e-7f8d-434f-9012-4094ed4ec921",
   "metadata": {},
   "source": [
    "# In the previous part of code I tried to enforce assumption that ZP codes must be present each period of analysis. This gives a low number of observations. Therefore, I run another assumption, when county must be present each perior, regardles if it consists of same ZIP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "5f7f6c12-26e9-4fde-911c-80d6e4e8be72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's see how many counties remain without any bundle (I called them Phoenix, just because)\n",
    "    \n",
    "phoenix_matches_sorted = final_matched_counties_F.sort_values(by=\"NAME\", ascending=False).reset_index(drop=True).drop(columns=['geometry'], errors=\"ignore\")\n",
    "phoenix_all_sorted = final_all_counties2.sort_values(by=\"NAME\", ascending=False).reset_index(drop=True).drop(columns=['geometry'], errors=\"ignore\")\n",
    "\n",
    "# Columns to exclude from renaming\n",
    "exclude_cols = {\"NAME\", \"dma_name\", \"dma_name2\", \"pct_overlap\"}\n",
    "\n",
    "# Rename columns conditionally\n",
    "phoenix_all_sorted = phoenix_all_sorted.rename(columns={col: f\"{col}_ALL\" for col in phoenix_all_sorted.columns if col not in exclude_cols})\n",
    "phoenix_matches_sorted = phoenix_matches_sorted.rename(columns={col: f\"{col}_MERGED\" for col in phoenix_matches_sorted.columns if col not in exclude_cols})\n",
    "\n",
    "merged_phoenix = pd.merge(phoenix_matches_sorted,phoenix_all_sorted,on=[\"NAME\", \"dma_name\", \"dma_name2\", \"pct_overlap\"],how=\"outer\",indicator=True)\n",
    "\n",
    "merged_phoenix = merged_phoenix.drop(columns=[\"geometry_MATCH\", \"geometry_ALL\", \"geometry\"], errors=\"ignore\")\n",
    "\n",
    "# The following line can be deleted\n",
    "export_df = pd.read_stata(\"/Users/oliverraab/Desktop/CUNY/5_semester/Short_Project3/Analysis/ZIP_code_alternative_solution(contain_pyhon_file_with_DMA_calculation)/zip_duplicates_all_dmas_experiemntal_restricted_to_two.dta\")\n",
    "\n",
    "# Here I am merging the dataset to see if those counties that are not divided into bundles affect any ZIP code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "fc46610e-8a97-4c2b-89b1-6ed8ab148f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oliverraab/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3050: DtypeWarning: Columns (72,78,79,159,163,166,167,168,171,175,178,179,180,202,203,205,224,225,226,235,236,237,241,242,243,244,251,284,285,286,287,288,289,292,293,294,295,296,297,300,301) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "# --- Load data ---\n",
    "input_path = \"/Users/oliverraab/Desktop/CUNY/5_semester/Short_Project3/Factbooks/FINAL_DMA_CHANNELS.xlsx\"\n",
    "df_DMA = pd.read_excel(input_path)\n",
    "\n",
    "input2_path = \"/Users/oliverraab/Desktop/CUNY/5_semester/Short_Project3/Analysis/ZIP_code_alternative_solution(contain_pyhon_file_with_DMA_calculation)/FINAL_DATA_MERGED/final_dataset_2.csv\"\n",
    "df_FINAL_DATA = pd.read_csv(input2_path)\n",
    "\n",
    "dma_fp = \"/Users/oliverraab/Desktop/CUNY/5_semester/Short_Project3/Data/DMA_data/nielsen-dma-master/nielsen_dma.json\"\n",
    "zip_shapefile_fp = \"/Users/oliverraab/Desktop/CUNY/5_semester/Short_Project3/Data/ZIP_code_coordinates_new/tl_2020_us_zcta520/tl_2020_us_zcta520.shp\"\n",
    "\n",
    "# --- Load geodata ---\n",
    "dmas = gpd.read_file(dma_fp).to_crs(\"EPSG:4269\")\n",
    "zip_gdf_master = gpd.read_file(zip_shapefile_fp).set_crs(\"EPSG:4269\")\n",
    "\n",
    "# --- Filter ZIPs of interest ---\n",
    "zips_of_interest = export_df[\"zipcode\"].astype(str).unique()\n",
    "zip_gdf_filtered = zip_gdf_master[zip_gdf_master[\"ZCTA5CE20\"].isin(zips_of_interest)].copy()\n",
    "\n",
    "# --- Project to equal-area CRS for accurate area measurement ---\n",
    "dmas_proj = dmas.to_crs(\"EPSG:5070\")              # NAD83 / Conus Albers (square meters)\n",
    "zip_gdf_proj = zip_gdf_filtered.to_crs(\"EPSG:5070\")\n",
    "\n",
    "# --- Compute total ZIP area ---\n",
    "zip_gdf_proj[\"zip_area\"] = zip_gdf_proj.geometry.area\n",
    "\n",
    "# --- Compute intersections ---\n",
    "intersections = gpd.overlay(zip_gdf_proj, dmas_proj, how=\"intersection\")\n",
    "\n",
    "# --- Compute overlap area ---\n",
    "intersections[\"overlap_area\"] = intersections.geometry.area\n",
    "\n",
    "# --- Compute percent overlap ---\n",
    "intersections[\"pct_overlap\"] = (intersections[\"overlap_area\"] / intersections[\"zip_area\"]) * 100\n",
    "\n",
    "# --- Sort by ZIP and overlap area ---\n",
    "intersections_sorted = intersections.sort_values(by=[\"ZCTA5CE20\", \"overlap_area\"], ascending=[True, False])\n",
    "\n",
    "n_df = intersections_sorted\n",
    "zip_to_dma2 = n_df[[\"ZCTA5CE20\", \"dma1\", \"overlap_area\", \"pct_overlap\"]].rename(columns={\"ZCTA5CE20\": \"zipcode\", \"dma1\": \"DMA_NEW\", \"overlap_area\" : \"zip_overlap\", \"pct_overlap\" : \"zip_pct_overlap\"})\n",
    "\n",
    "zip_to_dma2 = zip_to_dma2.rename(columns={\"DMA_NEW\": \"dma_name\"})\n",
    "\n",
    "# --- Merge with your main dataset ---\n",
    "export_df3 = export_df.merge(zip_to_dma2, on=[\"zipcode\", \"dma_name\"], how=\"left\")\n",
    "\n",
    "export_df4 = export_df3[export_df3['assigned_bundle_1'].notna()]\n",
    "\n",
    "export_df5 = export_df4[~(export_df4['zip_pct_overlap'] < 50)]\n",
    "export_df5 = export_df5[export_df5['zip_pct_overlap'].notna()] #to try only\n",
    "\n",
    "\n",
    "export_df_in_case = export_df\n",
    "\n",
    "export_df = export_df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "0278d84a-b86b-4864-b79a-96513e0991f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Step 1: Sort \n",
    "export_df = export_df.sort_values(by=[\"NAME\", \"pct_overlap\", \"dma_name\", \"dma_name2\"]).reset_index(drop=True)\n",
    "    # Step 2: Merge: I need to merge ZIPs with border counties to see if there are any \n",
    "merged = pd.merge(export_df, merged_phoenix, on=[\"NAME\", \"pct_overlap\", \"dma_name\", \"dma_name2\"], how=\"left\", indicator=\"_merge2\")\n",
    "    # Step 3: Drop rows that exist only in merged_phoenix (\"_merge2\" == \"right_only\"): meaning, I am keeping only those ZIPs, that are in border counties\n",
    "merged = merged[merged[\"_merge2\"] != \"right_only\"]\n",
    "    # Step 4: BE READY TO CHANGE THIS ASSUMPTION IN THE NEXT CODE VARIATION: I assume that I need only those ZIP codes that are in all periods and are in fact in border counties. \n",
    "    # I can be less restrictive with the ZIP codes. \n",
    "merged_restricted = merged[(merged[\"_merge2\"] == 'both')]\n",
    "#Check the unmerged. If you have only both, continue with data cleaning\n",
    "#merged_restricted[\"_merge\"].unique() \n",
    "    # Step 5: I assign number to each category to find the unique bundles accross all DMAs\n",
    "merged_restricted = merged_restricted.copy()\n",
    "merged_restricted[\"dma_name2_num\"] = pd.factorize(merged_restricted[\"dma_name2\"])[0]\n",
    "\n",
    "merged_restricted[\"unique_bundle\"] = (merged_restricted[\"bundle_final\"].fillna(-1).astype(int).astype(str) + merged_restricted[\"dma_name2_num\"].astype(int).astype(str)).astype(int)\n",
    "\n",
    "sorted_unique_values = sorted(merged_restricted[\"unique_bundle\"].unique())\n",
    "unique_bundle_map = {val: i+1 for i, val in enumerate(sorted_unique_values)}\n",
    "merged_restricted[\"unique_bundle_sorted\"] = merged_restricted[\"unique_bundle\"].map(unique_bundle_map)\n",
    "\n",
    "#Find if the ZIP is part only of one side of the bundle. It can happend that all ZIPs in the bundle are <80, but not >15 or vice versa. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "merged_restricted_first = merged_restricted.drop_duplicates(subset=[\"unique_bundle_sorted\", \"class_final\"])\n",
    "\n",
    "# Count occurrences of each unique_bundle_sorted\n",
    "counts = merged_restricted_first[\"unique_bundle_sorted\"].value_counts()\n",
    "\n",
    "# Identify values that occur exactly once\n",
    "unique_bundles = counts[counts == 1].index\n",
    "\n",
    "# Keep only rows with unique_bundle_sorted that occur exactly once\n",
    "merged_restricted_first_filtered = merged_restricted_first[merged_restricted_first[\"unique_bundle_sorted\"].isin(unique_bundles)].copy()\n",
    "merged_restricted_first_filtered = merged_restricted_first_filtered[[\"class_premerge\", \"unique_bundle_sorted\"]].copy()\n",
    "merged_restricted_merged = pd.merge(merged_restricted, merged_restricted_first_filtered, on=[\"class_premerge\", \"unique_bundle_sorted\"], how=\"left\", indicator=\"_merge_flag\")\n",
    "\n",
    "#Here: merge_flag tells if the \n",
    "\n",
    "#Check if bundles are symmetrical: meaning, that one more than one bundle contains the same counties and ZIP codes\n",
    "\n",
    "\n",
    "\n",
    "merged_restricted_merged[\"new_var\"] = merged_restricted_merged[\"zipcode\"] + \" \" + merged_restricted_merged[\"NAME\"] + \" \" + merged_restricted_merged[\"dma_name\"]\n",
    "\n",
    "# Step 1: Group by 'unique_bundle_sorted' and collect the set of 'new_var' values\n",
    "bundle_to_newvar = merged_restricted_merged.groupby(\"unique_bundle_sorted\")[\"new_var\"].apply(lambda x: frozenset(x.unique()))\n",
    "\n",
    "# Step 2: Reverse the mapping: which bundles share the same set of new_var values\n",
    "newvar_to_bundles = bundle_to_newvar.reset_index().groupby(\"new_var\")[\"unique_bundle_sorted\"].apply(list)\n",
    "\n",
    "# Step 3: Mark bundles that share the same new_var set with another bundle\n",
    "shared_bundle_flags = bundle_to_newvar.map(lambda x: len(newvar_to_bundles[x]) > 1)\n",
    "\n",
    "# Step 4: Merge this info back into the original dataframe\n",
    "merged_restricted_merged = merged_restricted_merged.merge(\n",
    "    shared_bundle_flags.rename(\"duplicate_newvar_bundle_flag\"),\n",
    "    left_on=\"unique_bundle_sorted\", right_index=True\n",
    ")\n",
    "\n",
    "# Optional: Convert boolean to integer (1 if shared, 0 if not)\n",
    "merged_restricted[\"duplicate_newvar_bundle_flag\"] = merged_restricted_merged[\"duplicate_newvar_bundle_flag\"].astype(int)\n",
    "#merged_restricted.sort_values(by=[\"unique_bundle_sorted\"])\n",
    "\n",
    "\n",
    "merged_restricted_merged[\"duplicates\"] = merged_restricted_merged.duplicated(\"zipcode\", keep=False).astype(int)\n",
    "#merged_restricted.sort_values(by=[\"zipcode\"])\n",
    "\n",
    "\n",
    "\n",
    "#output_path = os.path.join(os.path.dirname(zip_flags_fp), \"merged_restricted.dta\")\n",
    "#merged_restricted_merged.to_stata(output_path, write_index=False)\n",
    "\n",
    "\n",
    "# Here I eliminate those observations that are symetrical and already in the bundle: I eliminated from 738 to 570 observations (with goal to get 314 unique zipcodes)\n",
    "\n",
    "#STEP 1: I eliminate all the lone observations that are duplicated by being assigned to symmetrical bundle\n",
    "\n",
    "\n",
    "\n",
    "# Assign duplication order: 0 for first, 1 for second, etc.\n",
    "merged_restricted_merged[\"duplicates2\"] = merged_restricted_merged.groupby(\"zipcode\").cumcount()\n",
    "\n",
    "merged_restricted_merged2 = merged_restricted_merged.drop(\n",
    "    merged_restricted_merged[\n",
    "        (merged_restricted_merged[\"duplicate_newvar_bundle_flag\"] == 1) &\n",
    "        (merged_restricted_merged[\"_merge_flag\"] == \"both\") &\n",
    "        (merged_restricted_merged[\"duplicates2\"] > 0)\n",
    "    ].index\n",
    ")\n",
    "\n",
    "\n",
    "len(merged_restricted_merged2)\n",
    "\n",
    "#STEP 2: I eliminate all the observations that are already assigned to full bundle that are duplicated by being assigned to symmetrical bundle: I eliminate up to 508 (goal is 314)\n",
    "\n",
    "merged_restricted_merged2 = merged_restricted_merged2.drop(columns=[\"duplicate_newvar_bundle_flag\", \"duplicates\", \"duplicates2\"], errors=\"ignore\")\n",
    "\n",
    "# Step 1: Group by 'unique_bundle_sorted' and collect the set of 'new_var' values\n",
    "bundle_to_newvar = merged_restricted_merged2.groupby(\"unique_bundle_sorted\")[\"new_var\"].apply(lambda x: frozenset(x.unique()))\n",
    "\n",
    "# Step 2: Reverse the mapping: which bundles share the same set of new_var values\n",
    "newvar_to_bundles = bundle_to_newvar.reset_index().groupby(\"new_var\")[\"unique_bundle_sorted\"].apply(list)\n",
    "\n",
    "# Step 3: Mark bundles that share the same new_var set with another bundle\n",
    "shared_bundle_flags = bundle_to_newvar.map(lambda x: len(newvar_to_bundles[x]) > 1)\n",
    "\n",
    "# Step 4: Merge this info back into the original dataframe\n",
    "merged_restricted_merged2 = merged_restricted_merged2.merge(\n",
    "    shared_bundle_flags.rename(\"duplicate_newvar_bundle_flag\"),\n",
    "    left_on=\"unique_bundle_sorted\", right_index=True\n",
    ")\n",
    "\n",
    "# Optional: Convert boolean to integer (1 if shared, 0 if not)\n",
    "#merged_restricted[\"duplicate_newvar_bundle_flag\"] = merged_restricted_merged[\"duplicate_newvar_bundle_flag\"].astype(int)\n",
    "#merged_restricted.sort_values(by=[\"unique_bundle_sorted\"])\n",
    "\n",
    "merged_restricted_merged2[\"duplicates\"] = merged_restricted_merged2.duplicated(\"zipcode\", keep=False).astype(int)\n",
    "#merged_restricted.sort_values(by=[\"zipcode\"])\n",
    "# Assign duplication order: 0 for first, 1 for second, etc.\n",
    "merged_restricted_merged2[\"duplicates2\"] = merged_restricted_merged.groupby(\"zipcode\").cumcount()\n",
    "\n",
    "output_path = os.path.join(os.path.dirname(zip_flags_fp), \"merged_restricted_merged2_UPDATED_50+strict.dta\")\n",
    "merged_restricted_merged2.to_stata(output_path, write_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "0813b5ad-f23d-43f7-8ce0-8634cef2d4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataset contains 6256 rows with ZIPs from sorted5.\n"
     ]
    }
   ],
   "source": [
    "merged_restricted_merged3 = merged_restricted_merged2.drop(\n",
    "    merged_restricted_merged2[\n",
    "        (merged_restricted_merged2[\"duplicate_newvar_bundle_flag\"] == 1) &\n",
    "        (merged_restricted_merged2[\"_merge_flag\"] == \"left_only\") &\n",
    "        (merged_restricted_merged2[\"duplicates2\"] > 0)\n",
    "    ].index\n",
    ")\n",
    "\n",
    "len(merged_restricted_merged3)\n",
    "\n",
    "len(merged_restricted_merged3[\"zipcode\"].unique())\n",
    "\n",
    "# STEP3: I have constructed an alghoritm to eliminate those ZIPs that are duplicated and are not symmetrical\n",
    "# The logic is to eliminate ZIP from the bundle that has surpluss of ZIPs\n",
    "\n",
    "\n",
    "# Here I am looking for correct way how to clean duplicated from the data \n",
    "\n",
    "merged_restricted_unmatched = merged_restricted_merged3[merged_restricted_merged3[\"_merge_flag\"] != \"both\"].copy()\n",
    "counts = merged_restricted_unmatched.groupby([\"unique_bundle_sorted\", \"class_final\"])[\"zipcode\"].nunique().reset_index(name=\"zip_count\")\n",
    "dup_zip_df = merged_restricted_unmatched[merged_restricted_unmatched.duplicated(\"zipcode\", keep=False)].copy()\n",
    "dup_zip_df = dup_zip_df.merge(counts, on=[\"unique_bundle_sorted\", \"class_final\"], how=\"left\")\n",
    "\n",
    "# Initialize the new column with 0\n",
    "dup_zip_df['is_max_zip_count'] = 0\n",
    "\n",
    "# Loop through each unique ZIP code\n",
    "for zip_code in dup_zip_df['zipcode'].unique():\n",
    "    subset = dup_zip_df[dup_zip_df['zipcode'] == zip_code]\n",
    "    max_zip_count = subset['zip_count'].max()\n",
    "    \n",
    "    # Check if the max occurs only once (no tie)\n",
    "    if (subset['zip_count'] == max_zip_count).sum() == 1:\n",
    "        # Find the index of the unique max and set it to 1\n",
    "        idx = subset[subset['zip_count'] == max_zip_count].index[0]\n",
    "        dup_zip_df.loc[idx, 'is_max_zip_count'] = 1\n",
    "\n",
    "dup_zip_df = dup_zip_df[['zipcode', 'class_final', 'unique_bundle_sorted', 'zip_count', 'is_max_zip_count']]\n",
    "merged_restricted_unmatched = merged_restricted_unmatched.drop(columns=['is_max_zip_count', 'is_max_zip_count_x', 'is_max_zip_count_y'], errors=\"ignore\")\n",
    "merged_restricted_unmatched = merged_restricted_unmatched.merge(dup_zip_df, on=['zipcode', 'class_final', 'unique_bundle_sorted'], how=\"left\")\n",
    "\n",
    "\n",
    "merged_restricted_unmatched2 = merged_restricted_unmatched[merged_restricted_unmatched['is_max_zip_count'] != 1]\n",
    "counts2 = merged_restricted_unmatched2.groupby([\"unique_bundle_sorted\", \"class_final\"])[\"zipcode\"].nunique().reset_index(name=\"zip_count2\")\n",
    "dup_zip_df2 = merged_restricted_unmatched2[merged_restricted_unmatched2.duplicated(\"zipcode\", keep=False)].copy()\n",
    "dup_zip_df2 = dup_zip_df2.drop(columns=['zip_count2', 'zip_count2_x', 'zip_count2_y'], errors=\"ignore\")\n",
    "dup_zip_df2 = dup_zip_df2.merge(counts2, on=[\"unique_bundle_sorted\", \"class_final\"], how=\"left\")\n",
    "\n",
    "# Initialize the new column with 0\n",
    "dup_zip_df2['is_max_zip_count2'] = 0\n",
    "\n",
    "# Loop through each unique ZIP code\n",
    "for zip_code in dup_zip_df2['zipcode'].unique():\n",
    "    subset = dup_zip_df2[dup_zip_df2['zipcode'] == zip_code]\n",
    "    max_zip_count2 = subset['zip_count2'].max()\n",
    "    \n",
    "    # Check if the max occurs only once (no tie)\n",
    "    if (subset['zip_count2'] == max_zip_count2).sum() == 1:\n",
    "        # Find the index of the unique max and set it to 1\n",
    "        idx = subset[subset['zip_count2'] == max_zip_count2].index[0]\n",
    "        dup_zip_df2.loc[idx, 'is_max_zip_count2'] = 1\n",
    "\n",
    "dup_zip_df2 = dup_zip_df2[['zipcode', 'class_final', 'unique_bundle_sorted', 'zip_count2', 'is_max_zip_count2']]\n",
    "merged_restricted_unmatched = merged_restricted_unmatched.drop(columns=['is_max_zip_count2_x', 'is_max_zip_count2_y', 'is_max_zip_count2'], errors=\"ignore\")\n",
    "merged_restricted_unmatched = merged_restricted_unmatched.merge(dup_zip_df2, on=['zipcode', 'class_final', 'unique_bundle_sorted'], how=\"left\")\n",
    "\n",
    "len(merged_restricted_unmatched[(merged_restricted_unmatched[\"is_max_zip_count\"] != 1) & (merged_restricted_unmatched[\"is_max_zip_count2\"] != 1)].sort_values(by=[\"zipcode\"]))\n",
    "\n",
    "new_duplicates = merged_restricted_unmatched[(merged_restricted_unmatched[\"is_max_zip_count\"] != 1) & (merged_restricted_unmatched[\"is_max_zip_count2\"] != 1)].sort_values(by=[\"zipcode\"])\n",
    "\n",
    "len(new_duplicates[\"zipcode\"].unique())\n",
    "\n",
    "# STEP4: The last elimination of the duplicates\n",
    "\n",
    "\n",
    "\n",
    "new_duplicates[\"duplicates\"] = new_duplicates.duplicated(\"zipcode\", keep=False).astype(int)\n",
    "sorted2=new_duplicates.sort_values(by=[\"unique_bundle_sorted\", \"class_final\"])\n",
    "\n",
    "#sorted2[[\"zipcode\", \"bundle_final\", \"class_final\", \"unique_bundle_sorted\", \"duplicates\"]]\n",
    "\n",
    "# Step 1: Group by unique_bundle_sorted and check if all duplicates == 1\n",
    "group_all_duplicates = sorted2.groupby(\"unique_bundle_sorted\")[\"duplicates\"].transform(lambda x: int((x == 1).all()))\n",
    "\n",
    "# Step 2: Assign to a new column\n",
    "sorted2[\"all_duplicates_in_group\"] = group_all_duplicates\n",
    "#sorted2[[\"zipcode\", \"bundle_final\", \"class_final\", \"unique_bundle_sorted\", \"duplicates\", \"all_duplicates_in_group\"]]\n",
    "#sorted2[[\"zipcode\", \"bundle_final\", \"class_final\", \"unique_bundle_sorted\", \"duplicates\", \"all_duplicates_in_group\"]].sort_values(by=\"zipcode\")\n",
    "sorted2[\"zipcode\"].nunique()\n",
    "\n",
    "sorted2[\"drop_stage_one\"] = ((sorted2[\"duplicates\"] == 1) & (sorted2[\"all_duplicates_in_group\"] == 1)).astype(int)\n",
    "#sorted2\n",
    "\n",
    "# Start with the original DataFrame\n",
    "df = new_duplicates.copy()\n",
    "\n",
    "# Initialize a variable to control the loop\n",
    "rows_dropped = True\n",
    "\n",
    "# Loop until no ZIPs are dropped\n",
    "while rows_dropped:\n",
    "    # Step 1: Mark duplicate ZIP codes\n",
    "    df[\"duplicates\"] = df.duplicated(\"zipcode\", keep=False).astype(int)\n",
    "    df = df.sort_values(by=[\"unique_bundle_sorted\", \"class_final\"])\n",
    "    \n",
    "    # Step 2: Check if all ZIPs in group are duplicates\n",
    "    df[\"all_duplicates_in_group\"] = df.groupby(\"unique_bundle_sorted\")[\"duplicates\"].transform(lambda x: int((x == 1).all()))\n",
    "    \n",
    "    # Step 3: Create drop_stage_one flag\n",
    "    df[\"drop_stage_one\"] = ((df[\"duplicates\"] == 1) & (df[\"all_duplicates_in_group\"] == 1)).astype(int)\n",
    "\n",
    "    # Step 4: Identify the first bundle to drop (if any)\n",
    "    bundles_to_drop = df.loc[df[\"drop_stage_one\"] == 1, \"unique_bundle_sorted\"].unique()\n",
    "    \n",
    "    if len(bundles_to_drop) > 0:\n",
    "        # Only drop the *first* bundle (lowest-numbered one)\n",
    "        first_bundle = bundles_to_drop.min()\n",
    "        rows_before = len(df)\n",
    "        df = df[df[\"unique_bundle_sorted\"] != first_bundle].copy()\n",
    "        rows_after = len(df)\n",
    "        rows_dropped = rows_before != rows_after  # Update condition\n",
    "    else:\n",
    "        rows_dropped = False  # No rows to drop → end loop\n",
    "\n",
    "# Final result in `df`\n",
    "sorted3 = df.copy()\n",
    "\n",
    "sorted4 = sorted3[sorted3[\"duplicates\"] == 1][[\"zipcode\", \"unique_bundle_sorted\", \"class_final\"]]\n",
    "\n",
    "#sorted4.sort_values(by=[\"zipcode\"])\n",
    "\n",
    "sorted5 = sorted3.merge(sorted4, on=['zipcode', 'class_final', 'unique_bundle_sorted'], indicator=\"_merge_last\", how=\"left\")\n",
    "\n",
    "len(sorted5)\n",
    "\n",
    "sorted5['zipcode'] = sorted5['zipcode'].astype(str).str.zfill(5)\n",
    "\n",
    "# Step 2: Load the full cleaned dataset from Stata\n",
    "cleaned_data_fp = \"/Users/oliverraab/Desktop/CUNY/5_semester/Short_Project3/Analysis/Merged_Data_Final_Final.dta\"\n",
    "full_df = pd.read_stata(cleaned_data_fp)\n",
    "\n",
    "full_df['zipcode'] = full_df['zipcode'].astype(str).str.zfill(5)\n",
    "\n",
    "# Step 3: Filter full_df to only ZIP codes in sorted5\n",
    "zipcodes_to_keep = sorted5['zipcode'].unique()\n",
    "filtered_df = full_df[full_df['zipcode'].isin(zipcodes_to_keep)].copy()\n",
    "\n",
    "print(f\"Filtered dataset contains {len(filtered_df)} rows with ZIPs from sorted5.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "37573f4e-4a03-4b60-9088-b2886a85adcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataset contains 4519 rows from bundles with both <15 and >80.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Identify bundles with both <15 and >80 in sorted5\n",
    "bundle_class_counts = sorted5.pivot_table(index=\"unique_bundle_sorted\", \n",
    "                                          columns=\"class_final\", \n",
    "                                          aggfunc=\"size\", \n",
    "                                          fill_value=0)\n",
    "\n",
    "# Get bundles that have at least one <15 and one >80\n",
    "valid_bundles = bundle_class_counts[\n",
    "    (bundle_class_counts.get(\"<15\", 0) > 0) & \n",
    "    (bundle_class_counts.get(\">80\", 0) > 0)\n",
    "].index\n",
    "\n",
    "# Step 2: Subset sorted5 to only valid bundles\n",
    "sorted5_valid = sorted5[sorted5[\"unique_bundle_sorted\"].isin(valid_bundles)].copy()\n",
    "\n",
    "# Step 3: Load full dataset\n",
    "cleaned_data_fp = \"/Users/oliverraab/Desktop/CUNY/5_semester/Short_Project3/Analysis/Merged_Data_Final_Final.dta\"\n",
    "full_df = pd.read_stata(cleaned_data_fp)\n",
    "full_df['zipcode'] = full_df['zipcode'].astype(str).str.zfill(5)\n",
    "\n",
    "# Step 4: Keep ZIP codes from sorted5_valid only\n",
    "zipcodes_to_keep = sorted5_valid['zipcode'].unique()\n",
    "filtered_df = full_df[full_df['zipcode'].isin(zipcodes_to_keep)].copy()\n",
    "\n",
    "# Output\n",
    "print(f\"Filtered dataset contains {len(filtered_df)} rows from bundles with both <15 and >80.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "d33ebd09-9f0f-44e3-b4ab-0062ed402b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#. dis 5854/12372 = 0.47316521\n",
    "#. updated: 5429 if 50\n",
    "#. updated: 5517 if 30\n",
    "#. updated: 4519 if 50 Strict\n",
    "#. updated: 4672 if 30 Strict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164016a5-8df0-41fd-97ae-82c760677950",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
